//! SurrealStore - Direct SurrealDB Backend Implementation
//!
//! This module provides the primary and only database backend for NodeSpace,
//! using SurrealDB embedded database with RocksDB storage engine.
//!
//! # Architecture
//!
//! SurrealStore uses a **hub-and-spoke architecture** (Issue #560):
//! 1. **Hub `node` table** - Universal metadata for ALL node types with SCHEMAFULL validation
//! 2. **Spoke tables** - Type-specific data (task, date, schema) with bidirectional Record Links
//! 3. **Graph edges** - `has_child` edges for hierarchy, `mentions` for references
//! 4. **Record Links** - `node.data` → spoke, `spoke.node` → hub (composition, not RELATE)
//!
//! # Design Principles
//!
//! 1. **Embedded RocksDB**: Desktop-only backend using `kv-rocksdb` engine
//! 2. **SCHEMAFULL + FLEXIBLE**: Core fields strictly typed, user extensions allowed (Issue #560)
//! 3. **Record IDs**: Native SurrealDB format `node:uuid` (type embedded in ID)
//! 4. **Hub-and-Spoke**: Universal `node` table + spoke tables (task, date, schema) with bidirectional Record Links
//! 5. **Graph Edges**: Hierarchy via `has_child` edges (no parent_id/root_id fields)
//! 6. **Direct Access**: No abstraction layers, SurrealStore used directly by services
//!
//! # Performance Targets (from PoC)
//!
//! - Startup time: <100ms (PoC: 52ms)
//! - 100K nodes query: <200ms (PoC: 104ms)
//! - Deep pagination: <50ms (PoC: 8.3ms)
//! - Complex queries avg: <300ms (PoC: 211ms)
//!
//! # Examples
//!
//! ```rust,no_run
//! use nodespace_core::db::SurrealStore;
//! use std::path::PathBuf;
//!
//! #[tokio::main]
//! async fn main() -> anyhow::Result<()> {
//!     // Create embedded SurrealDB store
//!     let db_path = PathBuf::from("./data/surreal.db");
//!     let store = SurrealStore::new(db_path).await?;
//!
//!     // Direct database access
//!     let node = store.get_node("task:550e8400-e29b-41d4-a716-446655440000").await?;
//!
//!     Ok(())
//! }
//! ```

use crate::db::events::DomainEvent;
use crate::db::fractional_ordering::FractionalOrderCalculator;
use crate::models::{DeleteResult, Node, NodeQuery, NodeUpdate};
use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use serde_json::Value;
use std::path::PathBuf;
use std::sync::Arc;
use surrealdb::engine::local::{Db, RocksDb};
use surrealdb::engine::remote::http::{Client, Http};
use surrealdb::opt::auth::Root;
use surrealdb::sql::{Id, Thing};
use surrealdb::Surreal;
use tokio::sync::broadcast;
use tracing::warn;

/// Broadcast channel capacity for domain events.
///
/// 128 provides sufficient headroom for burst operations (bulk node creation)
/// while limiting memory overhead. Observer lag is acceptable - we only track
/// the current state, not historical events.
const DOMAIN_EVENT_CHANNEL_CAPACITY: usize = 128;

/// Represents a has_child edge from the database
///
/// Used for bulk loading the tree structure on startup.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EdgeRecord {
    /// Edge ID in SurrealDB format (e.g., "has_child:123")
    pub id: String,
    /// Parent node ID
    #[serde(rename = "in")]
    pub in_node: String,
    /// Child node ID
    #[serde(rename = "out")]
    pub out_node: String,
    /// Order position for this child in parent's children list
    pub order: f64,
}

/// Store operation types for automatic notification (Issue #718)
///
/// Used by the store-level notification system to indicate what type
/// of mutation occurred, enabling automatic event emission.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum StoreOperation {
    /// A new node was created
    Created,
    /// An existing node was updated
    Updated,
    /// A node was deleted
    Deleted,
}

/// Represents a store-level change notification (Issue #718)
///
/// Emitted automatically by store mutation methods when a registered
/// notifier is present. Contains all information needed for NodeService
/// to construct and broadcast domain events.
///
/// # Design Notes
///
/// - `source` is passed per-operation (not stored in SurrealStore) because
///   NodeService is a shared singleton - different clients pass their ID per-request
/// - For deleted nodes, `node` contains the node state before deletion
#[derive(Debug, Clone)]
pub struct StoreChange {
    /// The type of operation that occurred
    pub operation: StoreOperation,
    /// The node that was affected (for deletes, this is the pre-deletion state)
    pub node: Node,
    /// Optional client identifier for filtering events
    pub source: Option<String>,
}

/// Type alias for the store change notifier callback
///
/// This is a synchronous callback that runs immediately after store mutations.
/// The callback should be lightweight - heavy processing should be offloaded
/// to async tasks via channels.
pub type StoreNotifier = Arc<dyn Fn(StoreChange) + Send + Sync>;

// REMOVED: TYPES_WITH_SPOKE_TABLES constant (Issue #691)
// REMOVED: VALID_NODE_TYPES constant and validate_node_type() function (Issue #691)
//
// Both spoke table requirements and valid node types are now derived from
// schema definitions at runtime. See SurrealStore::build_schema_caches(),
// has_spoke_table(), and validate_node_type() methods.

/// Internal struct matching SurrealDB's schema
///
/// # Schema Evolution
///
/// - **v1.0** (Issue #470): Initial SurrealDB schema migration
///   - Core node fields
///   - Version-based optimistic concurrency control
///
/// - **v1.2** (Issue #511): Graph-native architecture
///   - Removed `uuid`, `parent_id`, `root_id`, `properties` fields
///   - Added `data` field: Optional record link to type-specific table
///   - Added `variants` field: Type history for lossless type switching
///   - Added `_schema_version` field: Universal versioning
///   - Table renamed from `nodes` to `node` (singular)
///   - Hierarchy via `has_child` graph edges only
///   - Only `task` type table exists (other types are schema-only)
///
/// - **v2.0** (Issue #729): Root-aggregate embedding architecture
///   - Removed `embedding_vector` and `embedding_stale` from node table
///   - Embeddings now stored in dedicated `embedding` table
///   - Only root nodes get embedded (subtree content aggregated)
#[derive(Debug, Clone, Serialize, Deserialize)]
struct SurrealNode {
    // Record ID is stored in the 'id' field returned by SurrealDB (e.g., node:⟨uuid⟩)
    id: Thing, // SurrealDB record ID (table:id format)
    node_type: String,
    content: String,
    version: i64,
    created_at: String,
    modified_at: String,
    #[serde(default)]
    mentions: Vec<String>,
    #[serde(default)]
    mentioned_by: Vec<String>,
    // Graph-native architecture fields (Issue #511)
    /// FETCH data Limitation (Issue #511):
    ///
    /// **Problem**: SurrealDB's Thing type cannot be deserialized to `serde_json::Value`.
    ///
    /// **Root Cause**: When using `SELECT * FROM node FETCH data`, the `data` field can be:
    /// - A String (record link): `"task:uuid"`  when not fetched
    /// - An Object (fetched record): `{id: Thing, priority: "HIGH", ...}` when FETCH succeeds
    ///
    /// The `id` field in the fetched object is a SurrealDB `Thing` type, which serde_json
    /// cannot deserialize to `Value`, causing:
    /// ```text
    /// Error: invalid type: enum, expected any valid JSON value
    /// ```
    ///
    /// **Attempted Solutions**:
    /// 1. ❌ Option<Value> - Fails with Thing deserialization error
    /// 2. ❌ Custom deserializer - Complex, error-prone
    /// 3. ✅ Skip + manual fetch with OMIT id - Current workaround
    ///
    /// **Workaround**: Use `#[serde(skip_deserializing)]` and manually fetch properties
    /// with `SELECT * OMIT id` to exclude the Thing-typed id field.
    ///
    /// **Performance Impact**: Creates N+1 query pattern (see get_children implementation).
    /// Batch fetching added to mitigate this issue.
    ///
    /// **Future**: Investigate SurrealDB support for FETCH with field exclusion:
    /// `SELECT * FROM node FETCH data.* OMIT data.id;`
    #[serde(skip_deserializing)]
    data: Option<Value>, // Placeholder - properties fetched separately
    #[serde(skip_deserializing, default)]
    variants: Value, // Type history map {task: "task:uuid", text: null}
    /// Properties field stores user-defined properties for types without dedicated tables
    /// For types with dedicated tables (task, schema), this contains _schema_version only
    /// and actual properties are fetched from the type-specific table
    #[serde(default)]
    properties: Value,
}

impl From<SurrealNode> for Node {
    fn from(sn: SurrealNode) -> Self {
        // Extract UUID from Thing record ID (e.g., node:⟨uuid⟩ -> uuid)
        let id = match &sn.id.id {
            Id::String(s) => {
                // Format is "node:uuid", extract the UUID part
                s.split(':').nth(1).unwrap_or(s).to_string()
            }
            _ => sn.id.id.to_string(),
        };

        // Extract properties:
        // 1. If data field is populated (types with dedicated tables like task/schema), use it
        // 2. Otherwise use properties field (types without dedicated tables like text/date)
        let properties = if let Some(Value::Object(ref obj)) = sn.data {
            tracing::debug!(
                "FETCH data populated for node {}: data has {} fields",
                id,
                obj.len()
            );
            // Remove the 'id' field and use remaining fields as properties
            let mut props = obj.clone();
            props.remove("id");
            Value::Object(props)
        } else if !sn.properties.is_null() {
            tracing::debug!(
                "Using properties field for node {} (type: {}): {} fields",
                id,
                sn.node_type,
                sn.properties.as_object().map(|o| o.len()).unwrap_or(0)
            );
            sn.properties
        } else {
            tracing::debug!(
                "No properties found for node {} (type: {})",
                id,
                sn.node_type
            );
            serde_json::json!({})
        };

        Node {
            id,
            node_type: sn.node_type,
            content: sn.content,
            version: sn.version,
            created_at: DateTime::parse_from_rfc3339(&sn.created_at)
                .map(|dt| dt.with_timezone(&Utc))
                .unwrap_or_else(|_| Utc::now()),
            modified_at: DateTime::parse_from_rfc3339(&sn.modified_at)
                .map(|dt| dt.with_timezone(&Utc))
                .unwrap_or_else(|_| Utc::now()),
            properties,
            mentions: sn.mentions,
            mentioned_by: sn.mentioned_by,
        }
    }
}

/// Batch fetch properties for multiple nodes of the same type
///
/// **Purpose**: Avoid N+1 query pattern when fetching properties for multiple nodes.
///
/// **Performance**:
/// - Old: 100 nodes = 100 individual queries
/// - New: 100 nodes = 1 batch query per type
///
/// # Arguments
///
/// * `db` - SurrealDB connection
/// * `node_type` - Type of nodes (e.g., "task", "schema")
/// * `node_ids` - Vector of node IDs to fetch properties for
///
/// # Returns
///
/// HashMap mapping node ID to its properties
async fn batch_fetch_properties<C: surrealdb::Connection>(
    db: &Surreal<C>,
    node_type: &str,
    node_ids: &[String],
) -> Result<std::collections::HashMap<String, Value>> {
    if node_ids.is_empty() {
        return Ok(std::collections::HashMap::new());
    }

    // Strategy: Query each spoke record individually using OMIT id pattern
    // This avoids Thing deserialization issues while keeping ID association clear
    // Performance: Still much better than N+1 since we batch the queries together
    let mut result = std::collections::HashMap::new();

    for node_id in node_ids {
        // Query spoke table using backtick-quoted ID (same pattern as get_node)
        // IDs with special characters (hyphens, etc.) need backtick-quoting in SurrealDB
        let query = format!("SELECT * OMIT id, node FROM {}:`{}`;", node_type, node_id);

        // Clone node_id so we own it
        let node_id_owned = node_id.clone();

        let mut response = db.query(&query).await.with_context(|| {
            format!(
                "Failed to fetch properties for type '{}' id '{}'",
                node_type, node_id_owned
            )
        })?;

        // Deserialize as generic Value
        let records: Vec<Value> = response.take(0).with_context(|| {
            format!(
                "Failed to parse property results for type '{}' id '{}'",
                node_type, node_id_owned
            )
        })?;

        // Take first record if exists
        if let Some(props) = records.into_iter().next() {
            result.insert(node_id_owned, props);
        }
    }

    Ok(result)
}

/// SurrealStore implements NodeStore trait for SurrealDB backend
///
/// Supports two connection modes:
/// - **Embedded RocksDB**: Desktop production mode (Surreal<Db>)
/// - **HTTP Client**: Dev-proxy mode (Surreal<Client>)
///
/// Uses hybrid dual-table architecture for optimal query performance.
/// Emits domain events via broadcast channel when data changes.
pub struct SurrealStore<C = Db>
where
    C: surrealdb::Connection,
{
    /// SurrealDB connection
    db: Arc<Surreal<C>>,
    /// Broadcast channel for domain events (128 subscriber capacity)
    event_tx: broadcast::Sender<DomainEvent>,
    /// Cache of node types that have spoke tables (derived from schema definitions)
    ///
    /// A type needs a spoke table if:
    /// 1. It's the `schema` type (structural, always has spoke table)
    /// 2. Its schema node has `fields.len() > 0`
    ///
    /// **Cache Population Strategy (Issue #704):**
    /// - **First launch (fresh DB)**: NodeService seeds schemas and populates cache incrementally
    ///   via `add_to_schema_cache()` - no database re-query needed
    /// - **Subsequent launches**: `build_schema_caches()` queries existing schema records once at startup
    ///
    /// Replaces the hardcoded TYPES_WITH_SPOKE_TABLES constant (Issue #691).
    types_with_spoke_tables: std::collections::HashSet<String>,
    /// Cache of all valid node types (derived from schema definitions)
    ///
    /// Contains all schema IDs from the database, used for validating
    /// node_type parameters in queries to prevent SQL injection.
    ///
    /// **Cache Population Strategy (Issue #704):**
    /// - **First launch (fresh DB)**: NodeService seeds schemas and populates cache incrementally
    ///   via `add_to_schema_cache()` - no database re-query needed
    /// - **Subsequent launches**: `build_schema_caches()` queries existing schema records once at startup
    ///
    /// Replaces the hardcoded VALID_NODE_TYPES constant (Issue #691).
    valid_node_types: std::collections::HashSet<String>,
    /// Optional notifier callback for store-level change notifications (Issue #718)
    ///
    /// When registered, this callback is invoked synchronously after every store
    /// mutation (create, update, delete). The notifier enables automatic domain
    /// event emission without manual emit calls in NodeService methods.
    ///
    /// Set via `set_notifier()` after construction.
    notifier: Option<StoreNotifier>,
}

/// Type alias for embedded RocksDB store
pub type EmbeddedStore = SurrealStore<Db>;

/// Type alias for HTTP client store
pub type HttpStore = SurrealStore<Client>;

impl SurrealStore<Db> {
    /// Create a new SurrealStore with embedded RocksDB backend
    ///
    /// # Arguments
    ///
    /// * `db_path` - Path to RocksDB database directory
    ///
    /// # Returns
    ///
    /// Initialized SurrealStore with schema setup complete
    ///
    /// # Errors
    ///
    /// Returns error if:
    /// - Database path is invalid
    /// - RocksDB initialization fails
    /// - Schema initialization fails
    ///
    /// # Examples
    ///
    /// ```rust,no_run
    /// # use nodespace_core::db::SurrealStore;
    /// # use std::path::PathBuf;
    /// # #[tokio::main]
    /// # async fn main() -> Result<(), Box<dyn std::error::Error>> {
    /// let store = SurrealStore::new(PathBuf::from("./data/surreal.db")).await?;
    /// # Ok(())
    /// # }
    /// ```
    pub async fn new(db_path: PathBuf) -> Result<Self> {
        // Initialize embedded RocksDb
        let db = Surreal::new::<RocksDb>(db_path)
            .await
            .context("Failed to initialize SurrealDB with RocksDB backend")?;

        // Use namespace and database
        db.use_ns("nodespace")
            .use_db("nodes")
            .await
            .context("Failed to set namespace/database")?;

        let db = Arc::new(db);

        // Initialize schema (create tables from schema.surql)
        // Note: Schema nodes are seeded by NodeService, not here (Issue #704)
        Self::initialize_schema(&db).await?;

        // Build schema caches from definitions (Issue #691)
        let (types_with_spoke_tables, valid_node_types) = Self::build_schema_caches(&db).await?;

        // Initialize broadcast channel for domain events
        let (event_tx, _) = broadcast::channel(DOMAIN_EVENT_CHANNEL_CAPACITY);

        Ok(Self {
            db,
            event_tx,
            types_with_spoke_tables,
            valid_node_types,
            notifier: None,
        })
    }
}

impl SurrealStore<Client> {
    /// Create HTTP client store for dev-proxy mode
    ///
    /// This connects to a remote SurrealDB server via HTTP API.
    /// Used by dev-proxy to enable Surrealist inspection while preserving business logic.
    ///
    /// # Arguments
    ///
    /// * `endpoint` - SurrealDB server address (e.g., "127.0.0.1:8000")
    /// * `namespace` - Database namespace (e.g., "nodespace")
    /// * `database` - Database name (e.g., "nodes")
    /// * `username` - Auth username (e.g., "root")
    /// * `password` - Auth password (e.g., "root")
    ///
    /// # Example
    ///
    /// ```rust,no_run
    /// use nodespace_core::db::SurrealStore;
    ///
    /// #[tokio::main]
    /// async fn main() -> anyhow::Result<()> {
    ///     let store = SurrealStore::new_http(
    ///         "127.0.0.1:8000",
    ///         "nodespace",
    ///         "nodes",
    ///         "root",
    ///         "root"
    ///     ).await?;
    ///
    ///     // Use store normally - same API as embedded mode
    ///     let node = store.get_node("some-id").await?;
    ///
    ///     Ok(())
    /// }
    /// ```
    pub async fn new_http(
        endpoint: &str,
        namespace: &str,
        database: &str,
        username: &str,
        password: &str,
    ) -> Result<Self> {
        tracing::info!("Connecting to SurrealDB HTTP server at {}", endpoint);

        // Create HTTP client connection to remote SurrealDB server
        let db = Surreal::new::<Http>(endpoint)
            .await
            .context("Failed to connect to SurrealDB HTTP server")?;

        // Authenticate with root credentials
        db.signin(Root { username, password })
            .await
            .context("Failed to authenticate with SurrealDB")?;

        // Set namespace and database
        db.use_ns(namespace)
            .use_db(database)
            .await
            .context("Failed to set namespace/database")?;

        let db = Arc::new(db);

        // Initialize schema tables (idempotent - uses IF NOT EXISTS)
        // Even in HTTP mode, we need to ensure schema exists for fresh databases
        // Note: Schema nodes are seeded by NodeService, not here (Issue #704)
        Self::initialize_schema(&db).await?;

        // Build schema caches from definitions (Issue #691)
        let (types_with_spoke_tables, valid_node_types) = Self::build_schema_caches(&db).await?;

        tracing::info!("✅ Connected to SurrealDB HTTP server");

        // Initialize broadcast channel for domain events
        let (event_tx, _) = broadcast::channel(DOMAIN_EVENT_CHANNEL_CAPACITY);

        Ok(Self {
            db,
            event_tx,
            types_with_spoke_tables,
            valid_node_types,
            notifier: None,
        })
    }
}

impl<C> SurrealStore<C>
where
    C: surrealdb::Connection,
{
    /// Set the store change notifier callback (Issue #718)
    ///
    /// Registers a callback that will be invoked synchronously after every store
    /// mutation. This enables automatic domain event emission from NodeService
    /// without manual emit calls in each method.
    ///
    /// # Arguments
    ///
    /// * `notifier` - Callback function that receives `StoreChange` on mutations
    ///
    /// # Example
    ///
    /// ```rust,ignore
    /// let store = SurrealStore::new(db_path).await?;
    /// store.set_notifier(Arc::new(|change| {
    ///     println!("Store change: {:?}", change.operation);
    /// }));
    /// ```
    pub fn set_notifier(&mut self, notifier: StoreNotifier) {
        self.notifier = Some(notifier);
    }

    /// Notify registered callback of a store change (Issue #718)
    ///
    /// Called internally by mutation methods after successful operations.
    /// Does nothing if no notifier is registered.
    fn notify(&self, change: StoreChange) {
        if let Some(notifier) = &self.notifier {
            notifier(change);
        }
    }

    /// Get the underlying database connection
    ///
    /// This is used by services (like SchemaService) that need direct database access
    /// for operations like DEFINE TABLE or DEFINE FIELD.
    pub fn db(&self) -> &Arc<Surreal<C>> {
        &self.db
    }

    /// Subscribe to domain events emitted by this store
    ///
    /// Returns a receiver that will get notified when nodes or edges change.
    /// Multiple subscribers are supported - each gets their own copy of events.
    ///
    /// # Example
    ///
    /// ```rust,no_run
    /// # use nodespace_core::db::SurrealStore;
    /// # use nodespace_core::DomainEvent;
    /// # use std::path::PathBuf;
    /// # #[tokio::main]
    /// # async fn main() -> Result<(), Box<dyn std::error::Error>> {
    /// # let store = SurrealStore::new(PathBuf::from("./data/test.db")).await?;
    /// let mut rx = store.subscribe_to_events();
    /// while let Ok(event) = rx.recv().await {
    ///     match event {
    ///         DomainEvent::NodeCreated { node_id, .. } => {
    ///             println!("Node created: {}", node_id)
    ///         }
    ///         DomainEvent::NodeUpdated { node_id, .. } => {
    ///             println!("Node updated: {}", node_id)
    ///         }
    ///         // ... handle other events
    ///         _ => {}
    ///     }
    /// }
    /// # Ok(())
    /// # }
    /// ```
    pub fn subscribe_to_events(&self) -> broadcast::Receiver<DomainEvent> {
        self.event_tx.subscribe()
    }

    // Note: emit_event method removed - domain events are now emitted at NodeService layer
    // for client filtering support. See issue #665.

    /// Check if a node type has a spoke table
    ///
    /// A type needs a spoke table if:
    /// 1. It's the `schema` type (structural, DDL in schema.surql)
    /// 2. Its schema node has fields defined
    ///
    /// This replaces the hardcoded TYPES_WITH_SPOKE_TABLES constant (Issue #691).
    #[inline]
    pub fn has_spoke_table(&self, node_type: &str) -> bool {
        self.types_with_spoke_tables.contains(node_type)
    }

    /// Validates a node type against the schema-derived whitelist
    ///
    /// This prevents SQL injection attacks where malicious node_type values could
    /// alter query semantics. All node_type parameters used in dynamic queries
    /// must be validated with this method before use.
    ///
    /// Replaces the hardcoded VALID_NODE_TYPES constant (Issue #691).
    ///
    /// # Arguments
    /// * `node_type` - The node type string to validate
    ///
    /// # Returns
    /// * `Ok(())` if the node_type exists as a schema in the database
    /// * `Err(...)` if the node_type is not recognized
    fn validate_node_type(&self, node_type: &str) -> Result<()> {
        if self.valid_node_types.contains(node_type) {
            Ok(())
        } else {
            let valid_types: Vec<&String> = self.valid_node_types.iter().collect();
            Err(anyhow::anyhow!(
                "Invalid node type: '{}'. Valid types are: {:?}",
                node_type,
                valid_types
            ))
        }
    }

    /// Build both schema caches from database schema definitions
    ///
    /// Queries the `schema` spoke table to determine which node types exist and which
    /// have spoke tables (based on whether they have fields defined).
    ///
    /// # Returns
    ///
    /// - `types_with_spoke_tables`: Types that have spoke tables (schema + types with fields)
    /// - `valid_node_types`: All valid node types (all schema IDs)
    ///
    /// # Cache Population Strategy (Issue #704)
    ///
    /// **First launch (fresh database):**
    /// - Called during `SurrealStore::new()` but returns empty results (no schema records yet)
    /// - Cache starts with only {"schema"} (hardcoded)
    /// - NodeService then seeds schemas and populates cache via `add_to_schema_cache()`
    ///
    /// **Subsequent launches (existing database):**
    /// - Called during `SurrealStore::new()` and returns all existing schema records
    /// - Cache fully populated in one query: {"schema", "task", "text", "date", ...}
    /// - No further cache updates needed
    async fn build_schema_caches(
        db: &Arc<Surreal<C>>,
    ) -> Result<(
        std::collections::HashSet<String>,
        std::collections::HashSet<String>,
    )> {
        let mut spoke_tables = std::collections::HashSet::new();
        let mut valid_types = std::collections::HashSet::new();

        // Schema type always has a spoke table (structural, defined in schema.surql)
        // and is always a valid type
        spoke_tables.insert("schema".to_string());
        valid_types.insert("schema".to_string());

        // Query all schema nodes to get all valid types and which have fields
        // Schema nodes have node_type = "schema" and id = the type name (e.g., "task", "text")
        let query = r#"
            SELECT id, fields FROM schema;
        "#;

        let mut response = db
            .query(query)
            .await
            .context("Failed to query schema nodes for caches")?;

        // Parse results - each row has id (the type name) and fields
        #[derive(serde::Deserialize)]
        struct SchemaRow {
            id: surrealdb::sql::Thing,
            fields: Vec<serde_json::Value>,
        }

        let rows: Vec<SchemaRow> = response.take(0).unwrap_or_default();

        for row in rows {
            // Extract type name from Thing id (e.g., schema:task -> task)
            let type_name = match &row.id.id {
                surrealdb::sql::Id::String(s) => s.clone(),
                other => other.to_string(),
            };

            // All schema IDs are valid node types
            valid_types.insert(type_name.clone());

            // Types with fields need spoke tables
            if !row.fields.is_empty() {
                spoke_tables.insert(type_name);
            }
        }

        Ok((spoke_tables, valid_types))
    }

    /// Initialize database schema from schema.surql file (Issue #560)
    ///
    /// Creates SCHEMAFULL tables with FLEXIBLE fields for user extensions.
    /// Uses hub-and-spoke architecture with bidirectional Record Links.
    ///
    /// # Hub-and-Spoke Architecture
    /// - Hub: Universal `node` table with metadata for ALL nodes
    /// - Spokes: Type-specific tables (task, date, schema) with `node` reverse link
    /// - Graph edges: `has_child` and `mentions` relations for relationships
    /// - Record Links: Bidirectional pointers for composition (NOT RELATE)
    async fn initialize_schema(db: &Arc<Surreal<C>>) -> Result<()> {
        // Load schema from schema.surql file (Issue #560)
        // Hub-and-spoke architecture with SCHEMAFULL tables and Record Links
        let schema_sql = include_str!("schema.surql");

        db.query(schema_sql)
            .await
            .context("Failed to execute schema.surql")?;

        Ok(())
    }

    /// Parse SurrealDB Record ID into (table, uuid) components
    ///
    /// # Arguments
    ///
    /// * `record_id` - SurrealDB Record ID (e.g., "task:uuid")
    ///
    /// # Returns
    ///
    /// Tuple of (table_name, uuid_portion)
    #[allow(dead_code)]
    fn parse_record_id(record_id: &str) -> Result<(String, String)> {
        let parts: Vec<&str> = record_id.splitn(2, ':').collect();
        if parts.len() != 2 {
            return Err(anyhow::anyhow!(
                "Invalid Record ID format: {}. Expected 'table:uuid'",
                record_id
            ));
        }
        Ok((parts[0].to_string(), parts[1].to_string()))
    }

    /// Add a node type to schema caches (called during schema seeding)
    ///
    /// When NodeService seeds schema records on first launch, it populates the caches
    /// incrementally as each schema is created. This avoids re-querying the database
    /// after seeding - we already have the schema data in memory.
    ///
    /// # Arguments
    ///
    /// * `type_name` - The node type (e.g., "task", "text", "date")
    /// * `has_fields` - Whether this type has fields (determines if spoke table exists)
    ///
    /// # Cache Population Strategy (Issue #704)
    ///
    /// **First launch (fresh database):**
    /// ```text
    /// for schema in core_schemas {
    ///     create_schema_node_atomic(schema);
    ///     add_to_schema_cache(schema.id, !schema.fields.is_empty()); // ✅ No DB query
    /// }
    /// ```
    ///
    /// **Subsequent launches:**
    /// - Caches already populated by `build_schema_caches()` during `SurrealStore::new()`
    /// - This method is not called
    pub(crate) fn add_to_schema_cache(&mut self, type_name: String, has_fields: bool) {
        self.valid_node_types.insert(type_name.clone());
        if has_fields {
            self.types_with_spoke_tables.insert(type_name);
        }
    }
}

impl<C> SurrealStore<C>
where
    C: surrealdb::Connection,
{
    pub async fn create_node(&self, node: Node, source: Option<String>) -> Result<Node> {
        // Note: embedding_vector is not stored in hub-and-spoke architecture
        // Embeddings are managed separately for optimization

        // Check if we need to create a spoke record for properties
        let has_properties = !node
            .properties
            .as_object()
            .unwrap_or(&serde_json::Map::new())
            .is_empty();
        let should_create_spoke = self.has_spoke_table(&node.node_type);
        let props_with_schema = node.properties.as_object().cloned().unwrap_or_default();

        // Create hub node using simpler table:id syntax
        // Note: IDs with special characters (hyphens, spaces, etc.) need to be backtick-quoted
        // For types without spoke tables, store properties directly on the hub
        // Hub-spoke architecture: properties are NOT stored on hub node
        // They go to spoke tables (task, schema) for types that have them
        let hub_query = format!(
            r#"
            CREATE node:`{}` CONTENT {{
                node_type: $node_type,
                content: $content,
                version: $version,
                created_at: time::now(),
                modified_at: time::now(),
                mentions: [],
                mentioned_by: [],
                data: $data
            }};
        "#,
            node.id
        );

        let mut response = self
            .db
            .query(&hub_query)
            .bind(("node_type", node.node_type.clone()))
            .bind(("content", node.content.clone()))
            .bind(("version", node.version))
            .bind(("data", None::<String>))
            .await
            .context("Failed to create node in universal table")?;

        // Consume the CREATE response - critical for persistence
        let _: Result<Vec<serde_json::Value>, _> = response.take(0usize);

        // Verify the hub node was actually created by querying it back
        // This ensures the CREATE statement fully persisted before proceeding
        let verify_query = format!("SELECT * FROM node:`{}` LIMIT 1;", node.id);
        let mut verify_response = self
            .db
            .query(&verify_query)
            .await
            .context("Failed to verify hub node creation")?;

        let _: Vec<SurrealNode> = verify_response.take(0).context(format!(
            "Hub node '{}' was not created - verification query returned no results",
            node.id
        ))?;

        // Create spoke record if needed
        if should_create_spoke && has_properties {
            // CREATE spoke record using EXACTLY the same pattern as hub nodes (which works)
            // Use inline property assignments in the CONTENT block rather than passing $properties

            // Build the property assignments list
            let mut property_bindings = String::new();
            let mut binding_pairs = Vec::new();

            for (key, value) in props_with_schema.iter() {
                property_bindings.push_str(&format!("{}: ${},\n                ", key, key));
                binding_pairs.push((key.clone(), value.clone()));
            }

            // Remove trailing comma and newline
            property_bindings = property_bindings
                .trim_end_matches(",\n                ")
                .to_string();

            let create_spoke_query = format!(
                r#"
                CREATE {}:`{}` CONTENT {{
                    {}
                }};
                "#,
                node.node_type, node.id, property_bindings
            );

            let mut query_builder = self.db.query(&create_spoke_query);

            // Bind all property values using owned strings for keys
            for (key, value) in binding_pairs {
                query_builder = query_builder.bind((key, value));
            }

            let mut spoke_response = query_builder
                .await
                .context("Failed to create spoke record")?;

            // Consume the response to ensure it's fully executed
            // Use Option to handle Thing deserialization issues gracefully
            let _: Result<Vec<Option<serde_json::Value>>, _> = spoke_response.take(0);

            // Verify spoke record was created
            let verify_spoke = format!("SELECT * FROM {}:`{}` LIMIT 1;", node.node_type, node.id);
            let mut verify_response = self.db.query(&verify_spoke).await?;
            let _verify_results: Vec<serde_json::Value> =
                verify_response.take(0).unwrap_or_default();

            // Set bidirectional links: hub -> spoke and spoke -> hub
            let link_query = format!(
                r#"
                UPDATE node:`{}` SET data = {}:`{}`;
                UPDATE {}:`{}` SET node = node:`{}`;
            "#,
                node.id, node.node_type, node.id, node.node_type, node.id, node.id
            );

            let mut link_response = self
                .db
                .query(&link_query)
                .await
                .context("Failed to set spoke links")?;

            // Consume link responses - both UPDATE statements
            // UPDATE returns updated records which may have deserialization quirks
            let _: Result<Vec<serde_json::Value>, _> = link_response.take(0usize);
            let _: Result<Vec<serde_json::Value>, _> = link_response.take(1usize);
        }

        // Note: Parent-child relationships are now established separately via move_node()
        // This allows cleaner separation of node creation from hierarchy management

        // Notify registered callback of the store change (Issue #718)
        self.notify(StoreChange {
            operation: StoreOperation::Created,
            node: node.clone(),
            source,
        });

        // Return the created node directly
        Ok(node)
    }

    /// Create a child node atomically with parent edge in a single transaction
    ///
    /// This is the atomic version of create_node + move_node. It guarantees that either:
    /// - The node, type-specific record (if applicable), and parent edge are ALL created
    /// - OR nothing is created (transaction rolls back on failure)
    ///
    /// # Performance Target
    /// - <15ms for create operation (from Issue #532 acceptance criteria)
    ///
    /// # Arguments
    ///
    /// * `parent_id` - ID of the parent node
    /// * `node_type` - Type of the node to create
    /// * `content` - Content of the node
    /// * `properties` - Properties for the node
    ///
    /// # Returns
    ///
    /// The created node with all fields populated
    ///
    /// # Example
    ///
    /// ```rust,no_run
    /// # use nodespace_core::db::SurrealStore;
    /// # use serde_json::json;
    /// # async fn example(store: &SurrealStore) -> anyhow::Result<()> {
    /// let child = store.create_child_node_atomic(
    ///     "parent-uuid",
    ///     "text",
    ///     "Child content",
    ///     json!({}),
    ///     None,
    /// ).await?;
    /// # Ok(())
    /// # }
    /// ```
    pub async fn create_child_node_atomic(
        &self,
        parent_id: &str,
        node_type: &str,
        content: &str,
        properties: Value,
        source: Option<String>,
    ) -> Result<Node> {
        use uuid::Uuid;

        // Validate node type to prevent SQL injection
        self.validate_node_type(node_type)?;

        // Generate node ID and convert parameters to owned strings for 'static lifetime
        let node_id = Uuid::new_v4().to_string();
        let parent_id = parent_id.to_string();
        let node_type = node_type.to_string();
        let content = content.to_string();

        // Validate parent exists (prevent orphan nodes)
        let parent_exists = self.get_node(&parent_id).await?;
        if parent_exists.is_none() {
            return Err(anyhow::anyhow!("Parent node not found: {}", parent_id));
        }

        // Validate no cycle (prevent child from being ancestor of parent)
        self.validate_no_cycle(&parent_id, &node_id).await?;

        // Calculate fractional order for the new node
        // Get the last child's order value
        #[derive(Deserialize)]
        struct EdgeOrder {
            order: f64,
        }

        let parent_thing = surrealdb::sql::Thing::from(("node".to_string(), parent_id.clone()));
        let mut order_response = self
            .db
            .query(
                "SELECT order FROM has_child WHERE in = $parent_thing ORDER BY order DESC LIMIT 1;",
            )
            .bind(("parent_thing", parent_thing.clone()))
            .await
            .context("Failed to get last child order")?;

        let last_order: Option<EdgeOrder> = order_response
            .take(0)
            .context("Failed to extract last child order")?;

        let new_order = if let Some(edge) = last_order {
            FractionalOrderCalculator::calculate_order(Some(edge.order), None)
        } else {
            FractionalOrderCalculator::calculate_order(None, None)
        };

        // Prepare properties for type-specific tables
        let props_with_schema = properties.as_object().cloned().unwrap_or_default();
        let has_type_table = self.has_spoke_table(&node_type);

        // Prepare spoke properties WITHOUT the node field (we'll set it in the query using a Thing binding)
        // This is because JSON objects can't represent SurrealDB Record Links properly
        let spoke_properties = if has_type_table && !props_with_schema.is_empty() {
            Some(Value::Object(props_with_schema.clone()))
        } else {
            None
        };

        // Prepare hub properties for non-spoke types (text, date, etc.)
        let hub_properties = if !has_type_table {
            Some(properties.clone())
        } else {
            None
        };

        // Calculate fractional order using FractionalOrderCalculator (Issue #550)
        let order = new_order;

        // Build atomic transaction query with bidirectional Record Links (Issue #560)
        // This ensures ALL operations succeed or ALL fail
        // Hub-and-spoke: Create spoke with reverse link, then hub with forward link
        // Note: Use time::now() for datetime fields instead of binding string timestamps
        let transaction_query = if has_type_table && !props_with_schema.is_empty() {
            // For types with dedicated tables (task, schema) - bidirectional links
            // Use dynamic properties binding to support all spoke types (not just task)
            r#"
                BEGIN TRANSACTION;

                -- Step 1: Create spoke (type-specific data) with properties
                CREATE $type_id CONTENT $spoke_properties;

                -- Step 2: Set the reverse link (spoke.node -> hub) using proper Thing binding
                UPDATE $type_id SET node = $node_id;

                -- Step 3: Create hub (universal metadata) with forward link to spoke
                CREATE $node_id CONTENT {
                    id: $node_id,
                    node_type: $node_type,
                    content: $content,
                    data: $type_id,
                    version: 1,
                    created_at: time::now(),
                    modified_at: time::now()
                };

                -- Step 4: Create parent-child edge (parent->has_child->child)
                RELATE $parent_id->has_child->$node_id CONTENT {
                    order: $order,
                    created_at: time::now(),
                    version: 1
                };

                COMMIT TRANSACTION;
            "#
            .to_string()
        } else {
            // For types without dedicated tables (text, date, etc.) - no spoke needed
            // Store properties directly on the hub node
            r#"
                BEGIN TRANSACTION;

                -- Create hub only (no spoke needed for simple types)
                -- Properties stored directly on hub for non-spoke types
                CREATE $node_id CONTENT {
                    id: $node_id,
                    node_type: $node_type,
                    content: $content,
                    data: NONE,
                    properties: $hub_properties,
                    version: 1,
                    created_at: time::now(),
                    modified_at: time::now()
                };

                -- Create parent-child edge (parent->has_child->child)
                RELATE $parent_id->has_child->$node_id CONTENT {
                    order: $order,
                    created_at: time::now(),
                    version: 1
                };

                COMMIT TRANSACTION;
            "#
            .to_string()
        };

        // Construct Thing objects for Record IDs
        // Thing format: table name paired with ID
        let node_thing = surrealdb::sql::Thing::from(("node".to_string(), node_id.clone()));
        let type_thing = surrealdb::sql::Thing::from((node_type.clone(), node_id.clone()));

        // Execute transaction
        let mut query = self
            .db
            .query(transaction_query)
            .bind(("node_id", node_thing))
            .bind(("parent_id", parent_thing))
            .bind(("type_id", type_thing))
            .bind(("node_type", node_type.clone()))
            .bind(("content", content.clone()))
            .bind(("order", order));

        // Conditionally bind spoke_properties for types with dedicated tables
        if let Some(spoke_props) = spoke_properties {
            query = query.bind(("spoke_properties", spoke_props));
        }

        // Conditionally bind hub_properties for types without dedicated tables
        if let Some(hub_props) = hub_properties {
            query = query.bind(("hub_properties", hub_props));
        }

        let response = query.await.context(format!(
            "Failed to execute create child node transaction for '{}' under parent '{}'",
            node_id, parent_id
        ))?;

        // Check transaction response for errors
        // We need to consume results to ensure execution, but ignore serialization errors
        // that occur when SurrealDB returns internal types (like transaction markers)
        response.check().context(format!(
            "Transaction failed when creating child node '{}' under parent '{}'",
            node_id, parent_id
        ))?;

        // Fetch and return created node (ensures timestamps match database values)
        let node = self
            .get_node(&node_id)
            .await?
            .ok_or_else(|| anyhow::anyhow!("Node not found after creation for '{}'", node_id))?;

        // Notify registered callback of the store change (Issue #718)
        self.notify(StoreChange {
            operation: StoreOperation::Created,
            node: node.clone(),
            source,
        });

        Ok(node)
    }

    pub async fn get_node(&self, id: &str) -> Result<Option<Node>> {
        // Two-query approach: hub first, then spoke if needed
        //
        // For embedded SurrealDB (RocksDB), the extra in-process call is ~microseconds.
        // This approach scales with any number of spoke types without code changes.
        //
        // For compile-time type safety, use get_task_node() or get_schema_node().

        // Query 1: Get hub node
        let hub_query = format!("SELECT * OMIT id, data FROM node:`{id}` LIMIT 1;", id = id);
        let mut response = self
            .db
            .query(&hub_query)
            .await
            .context("Failed to query hub node")?;

        let results: Vec<Value> = response.take(0).unwrap_or_default();
        let Some(hub) = results.into_iter().next() else {
            return Ok(None);
        };

        // Parse hub fields manually (SurrealDB snake_case → Node camelCase)
        let node_type = hub["node_type"].as_str().unwrap_or("text").to_string();

        // Query 2: Get spoke data if this type has a spoke table
        let has_spoke = self.has_spoke_table(&node_type);
        let properties = if has_spoke {
            let spoke_query = format!(
                "SELECT * OMIT id, node FROM {table}:`{id}` LIMIT 1;",
                table = node_type,
                id = id
            );
            let mut spoke_response = self
                .db
                .query(&spoke_query)
                .await
                .context("Failed to query spoke table")?;

            let spoke_results: Vec<Value> = spoke_response.take(0).unwrap_or_default();
            spoke_results
                .into_iter()
                .next()
                .unwrap_or_else(|| serde_json::json!({}))
        } else {
            hub.get("properties")
                .cloned()
                .unwrap_or(serde_json::json!({}))
        };

        let created_at = hub["created_at"]
            .as_str()
            .and_then(|s| DateTime::parse_from_rfc3339(s).ok())
            .map(|dt| dt.with_timezone(&Utc))
            .unwrap_or_else(|| {
                warn!(node_id = %id, "Missing or invalid created_at timestamp, using current time");
                Utc::now()
            });

        let modified_at = hub["modified_at"]
            .as_str()
            .and_then(|s| DateTime::parse_from_rfc3339(s).ok())
            .map(|dt| dt.with_timezone(&Utc))
            .unwrap_or_else(|| {
                warn!(node_id = %id, "Missing or invalid modified_at timestamp, using current time");
                Utc::now()
            });

        let mentions = hub["mentions"]
            .as_array()
            .map(|arr| {
                arr.iter()
                    .filter_map(|v| v.as_str().map(String::from))
                    .collect()
            })
            .unwrap_or_default();

        let mentioned_by = hub["mentioned_by"]
            .as_array()
            .map(|arr| {
                arr.iter()
                    .filter_map(|v| v.as_str().map(String::from))
                    .collect()
            })
            .unwrap_or_default();

        Ok(Some(Node {
            id: id.to_string(),
            node_type,
            content: hub["content"].as_str().unwrap_or("").to_string(),
            version: hub["version"].as_i64().unwrap_or(1),
            created_at,
            modified_at,
            properties,
            mentions,
            mentioned_by,
        }))
    }

    pub async fn update_node(
        &self,
        id: &str,
        update: NodeUpdate,
        source: Option<String>,
    ) -> Result<Node> {
        // Fetch current node
        let current = self
            .get_node(id)
            .await?
            .ok_or_else(|| anyhow::anyhow!("Node not found: {}", id))?;

        let updated_content = update.content.unwrap_or(current.content);
        let updated_node_type = update.node_type.unwrap_or(current.node_type.clone());

        // Merge properties if they're being updated
        // For types without dedicated tables (text, date, etc.), store properties directly in universal table
        //
        // NOTE: _schema_version is managed by NodeService, not SurrealStore.
        // NodeService adds _schema_version only for node types with schema fields.
        // Don't add/preserve _schema_version here - it causes properties pollution.
        let properties_update = if let Some(ref updated_props) = update.properties {
            let mut merged_props = current.properties.as_object().cloned().unwrap_or_default();
            if let Some(new_props) = updated_props.as_object() {
                for (key, value) in new_props {
                    merged_props.insert(key.clone(), value.clone());
                }
            }
            // Properties are merged as-is - no automatic _schema_version insertion
            Some(serde_json::Value::Object(merged_props))
        } else {
            None
        };

        // Build update query - include properties if they're being updated
        let (query, bind_properties) = if properties_update.is_some() {
            (
                "
                UPDATE type::thing('node', $id) SET
                    content = $content,
                    node_type = $node_type,
                    modified_at = time::now(),
                    version = version + 1,
                    properties = $properties;
            ",
                true,
            )
        } else {
            (
                "
                UPDATE type::thing('node', $id) SET
                    content = $content,
                    node_type = $node_type,
                    modified_at = time::now(),
                    version = version + 1;
            ",
                false,
            )
        };

        let mut query_builder = self
            .db
            .query(query)
            .bind(("id", id.to_string()))
            .bind(("content", updated_content))
            .bind(("node_type", updated_node_type.clone()));

        if bind_properties {
            query_builder = query_builder.bind(("properties", properties_update.unwrap()));
        }

        query_builder.await.context("Failed to update node")?;

        // If properties were provided and node type has type-specific table, update it there too
        if let Some(updated_props) = update.properties {
            if self.has_spoke_table(&updated_node_type) {
                // UPSERT with MERGE to preserve existing spoke data on type reconversions
                // Scenario: text→task creates task:uuid, task→text preserves it, text→task reconnects
                // MERGE ensures old task properties (priority, due_date) aren't lost on reconversion
                // Only adds missing defaults, preserves user-set values
                // Note: Schema properties stored directly - enums handled via strong typing on read

                self.db
                    .query("UPSERT type::thing($table, $id) MERGE $properties;")
                    .bind(("table", updated_node_type.clone()))
                    .bind(("id", id.to_string()))
                    .bind(("properties", updated_props))
                    .await
                    .context("Failed to upsert properties in type-specific table")?;

                // Ensure data link exists (in case this is a type change)
                self.db
                    .query(
                        "UPDATE type::thing('node', $id) SET data = type::thing($type_table, $id);",
                    )
                    .bind(("id", id.to_string()))
                    .bind(("type_table", updated_node_type))
                    .await
                    .context("Failed to set data link")?;
            }
        }

        // Fetch and return updated node
        let updated_node = self
            .get_node(id)
            .await?
            .ok_or_else(|| anyhow::anyhow!("Node not found after update"))?;

        // Notify registered callback of the store change (Issue #718)
        self.notify(StoreChange {
            operation: StoreOperation::Updated,
            node: updated_node.clone(),
            source,
        });

        Ok(updated_node)
    }

    /// Update a schema node and execute DDL statements atomically
    ///
    /// When a schema node is updated, both the node data AND the corresponding
    /// SurrealDB table definitions must change together. This method ensures
    /// atomicity by wrapping both operations in a single transaction.
    ///
    /// # Arguments
    ///
    /// * `id` - The schema node ID (also the table name, e.g., "person", "task")
    /// * `update` - The node update to apply
    /// * `ddl_statements` - DDL statements to execute (DEFINE TABLE, DEFINE FIELD, etc.)
    ///
    /// # Returns
    ///
    /// The updated schema node
    ///
    /// # Errors
    ///
    /// Returns error if:
    /// - Node not found
    /// - DDL execution fails
    /// - Transaction fails
    ///
    /// # Example
    ///
    /// ```rust,no_run
    /// # use nodespace_core::db::SurrealStore;
    /// # use nodespace_core::NodeUpdate;
    /// # async fn example(store: &SurrealStore) -> anyhow::Result<()> {
    /// let ddl = vec![
    ///     "DEFINE TABLE IF NOT EXISTS person SCHEMAFULL;".to_string(),
    ///     "DEFINE FIELD IF NOT EXISTS name ON person TYPE string;".to_string(),
    /// ];
    /// let update = NodeUpdate::new().with_properties(serde_json::json!({"schema": "..."}));
    /// store.update_schema_node_atomic("person", update, ddl, None).await?;
    /// # Ok(())
    /// # }
    /// ```
    pub async fn create_schema_node_atomic(
        &self,
        node: Node,
        ddl_statements: Vec<String>,
        source: Option<String>,
    ) -> Result<Node> {
        // Validate this is a schema node
        if node.node_type != "schema" {
            return Err(anyhow::anyhow!(
                "create_schema_node_atomic only accepts schema nodes, got '{}'",
                node.node_type
            ));
        }

        // Build atomic transaction: DDL FIRST, then CREATE node + CREATE spoke
        // DDL must come before CREATE so the spoke table exists when we create schema entries
        let mut transaction_parts = vec!["BEGIN TRANSACTION;".to_string()];

        // Add all DDL statements FIRST (for the type this schema defines, e.g., task spoke table)
        for ddl in &ddl_statements {
            transaction_parts.push(ddl.clone());
        }

        // Create hub node
        transaction_parts.push(format!(
            r#"CREATE node:`{}` CONTENT {{
                node_type: $node_type,
                content: $content,
                version: 1,
                created_at: time::now(),
                modified_at: time::now(),
                mentions: [],
                mentioned_by: [],
                data: type::thing('schema', $id)
            }};"#,
            node.id
        ));

        // Create spoke record (schema table entry)
        transaction_parts.push(format!(
            r#"CREATE schema:`{}` CONTENT {{
                node: type::thing('node', $id),
                is_core: $is_core,
                version: $schema_version,
                description: $description,
                fields: $fields,
                relationships: $relationships
            }};"#,
            node.id
        ));

        transaction_parts.push("COMMIT TRANSACTION;".to_string());
        let transaction_query = transaction_parts.join("\n");

        // Extract schema-specific properties
        let is_core = node
            .properties
            .get("isCore")
            .and_then(|v| v.as_bool())
            .unwrap_or(false);
        let schema_version = node
            .properties
            .get("version")
            .and_then(|v| v.as_i64())
            .unwrap_or(1);
        let description = node
            .properties
            .get("description")
            .and_then(|v| v.as_str())
            .unwrap_or("");
        let fields = node
            .properties
            .get("fields")
            .cloned()
            .unwrap_or(serde_json::json!([]));
        let relationships = node
            .properties
            .get("relationships")
            .cloned()
            .unwrap_or(serde_json::json!([]));

        // Execute atomic transaction
        self.db
            .query(&transaction_query)
            .bind(("id", node.id.clone()))
            .bind(("node_type", node.node_type.clone()))
            .bind(("content", node.content.clone()))
            .bind(("is_core", is_core))
            .bind(("schema_version", schema_version))
            .bind(("description", description.to_string()))
            .bind(("fields", fields))
            .bind(("relationships", relationships))
            .await
            .context("Failed to execute atomic schema creation transaction")?;

        // Fetch and return the created node
        let created_node = self
            .get_node(&node.id)
            .await?
            .ok_or_else(|| anyhow::anyhow!("Schema node not found after creation"))?;

        // Notify registered callback of the store change (Issue #718)
        self.notify(StoreChange {
            operation: StoreOperation::Created,
            node: created_node.clone(),
            source,
        });

        Ok(created_node)
    }

    pub async fn update_schema_node_atomic(
        &self,
        id: &str,
        update: NodeUpdate,
        ddl_statements: Vec<String>,
        source: Option<String>,
    ) -> Result<Node> {
        // Fetch current node to verify it exists and get current state
        let current = self
            .get_node(id)
            .await?
            .ok_or_else(|| anyhow::anyhow!("Schema node not found: {}", id))?;

        // Prepare updated values
        let updated_content = update.content.unwrap_or(current.content);
        let updated_node_type = update.node_type.unwrap_or(current.node_type.clone());

        // Merge properties if they're being updated
        let properties_update = if let Some(ref updated_props) = update.properties {
            let mut merged_props = current.properties.as_object().cloned().unwrap_or_default();
            if let Some(new_props) = updated_props.as_object() {
                for (key, value) in new_props {
                    merged_props.insert(key.clone(), value.clone());
                }
            }
            serde_json::Value::Object(merged_props)
        } else {
            current.properties.clone()
        };

        // Build the atomic transaction query
        // This wraps: node update + spoke table update + all DDL statements in one transaction
        let mut transaction_parts = vec!["BEGIN TRANSACTION;".to_string()];

        // Add node update statement (updates the hub node table)
        transaction_parts.push(
            r#"UPDATE type::thing('node', $id) SET
                content = $content,
                node_type = $node_type,
                modified_at = time::now(),
                version = version + 1,
                properties = $properties;"#
                .to_string(),
        );

        // CRITICAL: Also update the spoke table (schema:id) where properties are actually read from
        // get_node() reads properties from the spoke table for types with spoke tables
        transaction_parts
            .push(r#"UPSERT type::thing('schema', $id) MERGE $properties;"#.to_string());

        // Ensure data link exists
        transaction_parts.push(
            r#"UPDATE type::thing('node', $id) SET data = type::thing('schema', $id);"#.to_string(),
        );

        // Add all DDL statements
        for ddl in ddl_statements {
            transaction_parts.push(ddl);
        }

        transaction_parts.push("COMMIT TRANSACTION;".to_string());
        let transaction_query = transaction_parts.join("\n");

        // Execute the atomic transaction
        self.db
            .query(&transaction_query)
            .bind(("id", id.to_string()))
            .bind(("content", updated_content))
            .bind(("node_type", updated_node_type))
            .bind(("properties", properties_update.clone()))
            .await
            .context("Failed to execute atomic schema update transaction")?;

        // Fetch and return updated node
        let updated_node = self
            .get_node(id)
            .await?
            .ok_or_else(|| anyhow::anyhow!("Schema node not found after atomic update"))?;

        tracing::info!(
            "Atomically updated schema node '{}' with {} DDL statements",
            id,
            transaction_parts.len() - 3 // Exclude BEGIN, UPDATE, COMMIT
        );

        // Notify registered callback of the store change (Issue #718)
        self.notify(StoreChange {
            operation: StoreOperation::Updated,
            node: updated_node.clone(),
            source,
        });

        Ok(updated_node)
    }

    /// Switch a node's type atomically, preserving old type in variants map
    ///
    /// This is an atomic type-switching operation that guarantees:
    /// - Node type is updated
    /// - New type-specific record is created (if type has properties)
    /// - Old type is preserved in variants map for lossless recovery
    /// - All updates happen atomically (all or nothing)
    ///
    /// # Variants Map Pattern
    ///
    /// The variants map stores the history of type-specific record IDs:
    /// ```json
    /// {
    ///   "task": "task:uuid-123",
    ///   "text": null,
    ///   "person": "person:uuid-456"
    /// }
    /// ```
    ///
    /// This enables:
    /// - Lossless type switching (can restore old properties)
    /// - Type history tracking
    /// - Future multi-type node support
    ///
    /// # Arguments
    ///
    /// * `node_id` - ID of the node to switch
    /// * `new_type` - New node type
    /// * `new_properties` - Properties for the new type
    ///
    /// # Returns
    ///
    /// The updated node with new type
    ///
    /// # Example
    ///
    /// ```rust,no_run
    /// # use nodespace_core::db::SurrealStore;
    /// # use serde_json::json;
    /// # async fn example(store: &SurrealStore) -> anyhow::Result<()> {
    /// // Switch a text node to a task node
    /// let node = store.switch_node_type_atomic(
    ///     "node-uuid",
    ///     "task",
    ///     json!({"status": "TODO", "priority": "HIGH"}),
    ///     None,
    /// ).await?;
    /// # Ok(())
    /// # }
    /// ```
    pub async fn switch_node_type_atomic(
        &self,
        node_id: &str,
        new_type: &str,
        new_properties: Value,
        source: Option<String>,
    ) -> Result<Node> {
        // Validate new_type to prevent SQL injection
        self.validate_node_type(new_type)?;

        // Convert parameters to owned strings for 'static lifetime
        let node_id = node_id.to_string();
        let new_type = new_type.to_string();

        // Validate node exists
        let current_node = self
            .get_node(&node_id)
            .await?
            .ok_or_else(|| anyhow::anyhow!("Node not found: {}", node_id))?;

        let old_type = current_node.node_type.clone();

        // Build variants update: preserve old type, add new type
        // variants map format: {"task": "task:uuid", "text": null, ...}
        let old_type_record = if self.has_spoke_table(&old_type) {
            format!("{}:{}", old_type, node_id)
        } else {
            "null".to_string()
        };

        let new_type_record = if self.has_spoke_table(&new_type) {
            format!("{}:{}", new_type, node_id)
        } else {
            "null".to_string()
        };

        // Prepare properties
        // Note: Schema properties stored directly - enums handled via strong typing on read
        let props_with_schema = new_properties.as_object().cloned().unwrap_or_default();
        let has_new_type_table = self.has_spoke_table(&new_type);

        // Build atomic transaction using Thing parameters
        // Note: Field names use snake_case (node_type, modified_at) to match hub schema
        let transaction_query = if has_new_type_table && !props_with_schema.is_empty() {
            // New type has properties table
            r#"
                BEGIN TRANSACTION;

                -- Create new type-specific record
                CREATE $new_type_id CONTENT $properties;

                -- Set the reverse link (spoke.node -> hub) using proper Thing binding
                UPDATE $new_type_id SET node = $node_id;

                -- Update node type, variants map, and data link
                UPDATE $node_id SET
                    node_type = $new_type,
                    modified_at = time::now(),
                    version = version + 1,
                    variants[$old_type] = $old_type_record,
                    variants[$new_type] = $new_type_id,
                    data = $new_type_id;

                COMMIT TRANSACTION;
            "#
            .to_string()
        } else {
            // New type doesn't have properties table
            r#"
                BEGIN TRANSACTION;

                -- Update node type and variants map
                UPDATE $node_id SET
                    node_type = $new_type,
                    modified_at = time::now(),
                    version = version + 1,
                    variants[$old_type] = $old_type_record,
                    variants[$new_type] = $new_type_record,
                    data = NONE;

                COMMIT TRANSACTION;
            "#
            .to_string()
        };

        // Construct Thing objects for Record IDs
        let node_thing = surrealdb::sql::Thing::from(("node".to_string(), node_id.clone()));
        let new_type_thing = surrealdb::sql::Thing::from((new_type.clone(), node_id.clone()));

        // Execute transaction
        let response = self
            .db
            .query(transaction_query)
            .bind(("node_id", node_thing))
            .bind(("new_type_id", new_type_thing))
            .bind(("new_type", new_type.clone()))
            .bind(("old_type", old_type.clone()))
            .bind(("old_type_record", old_type_record))
            .bind(("new_type_record", new_type_record))
            .bind(("properties", Value::Object(props_with_schema)))
            .await
            .context(format!(
                "Failed to execute switch type transaction for node '{}'",
                node_id
            ))?;

        // Check transaction response for errors
        response.check().context(format!(
            "Transaction failed when switching node '{}' type from '{}' to '{}'",
            node_id, old_type, new_type
        ))?;

        // Fetch and return updated node
        let node = self
            .get_node(&node_id)
            .await?
            .ok_or_else(|| anyhow::anyhow!("Node not found after type switch for '{}'", node_id))?;

        // Notify registered callback of the store change (Issue #718)
        self.notify(StoreChange {
            operation: StoreOperation::Updated,
            node: node.clone(),
            source,
        });

        Ok(node)
    }

    /// Update a node with version check (optimistic locking)
    ///
    /// Only updates the node if its version matches the expected version.
    /// This provides atomic version-checked updates to prevent lost updates
    /// in concurrent scenarios.
    ///
    /// # Arguments
    ///
    /// * `id` - Node UUID to update
    /// * `expected_version` - Expected current version (for optimistic locking)
    /// * `update` - Fields to update
    ///
    /// # Returns
    ///
    /// * `Ok(Some(Node))` - Update succeeded, returns updated node
    /// * `Ok(None)` - Version mismatch, no update performed
    /// * `Err(_)` - Database error or node not found
    ///
    /// # Examples
    ///
    /// ```rust,no_run
    /// # use nodespace_core::db::SurrealStore;
    /// # use nodespace_core::models::NodeUpdate;
    /// # use std::path::PathBuf;
    /// # #[tokio::main]
    /// # async fn main() -> Result<(), Box<dyn std::error::Error>> {
    /// # let store = SurrealStore::new(PathBuf::from("./data/surreal.db")).await?;
    /// let update = NodeUpdate {
    ///     content: Some("Updated content".to_string()),
    ///     ..Default::default()
    /// };
    ///
    /// match store.update_node_with_version_check("node-id", 5, update, None).await? {
    ///     Some(node) => println!("Updated to version {}", node.version),
    ///     None => println!("Version mismatch - node was modified by another process"),
    /// }
    /// # Ok(())
    /// # }
    /// ```
    pub async fn update_node_with_version_check(
        &self,
        id: &str,
        expected_version: i64,
        update: NodeUpdate,
        source: Option<String>,
    ) -> Result<Option<Node>> {
        // Fetch current node to build update values
        let current = self
            .get_node(id)
            .await?
            .ok_or_else(|| anyhow::anyhow!("Node not found: {}", id))?;

        // Calculate new version for explicit binding
        let new_version = expected_version + 1;

        // Atomic update with version check using record ID
        // NOTE: Properties are NOT stored on hub node - they go to spoke tables
        // (hub-spoke architecture: hub has metadata, spokes have type-specific data)
        let query = "
            UPDATE type::thing('node', $id) SET
                content = $content,
                node_type = $node_type,
                modified_at = time::now(),
                version = $new_version
            WHERE version = $expected_version
            RETURN AFTER;
        ";

        let updated_content = update.content.unwrap_or(current.content);
        let updated_node_type = update.node_type.unwrap_or(current.node_type.clone());
        let updated_properties = update.properties.clone();

        let mut response = self
            .db
            .query(query)
            .bind(("id", id.to_string()))
            .bind(("expected_version", expected_version))
            .bind(("new_version", new_version))
            .bind(("content", updated_content))
            .bind(("node_type", updated_node_type.clone()))
            .await
            .context("Failed to update node with version check")?;

        // Extract updated nodes from response
        let updated_nodes: Vec<SurrealNode> = response
            .take(0)
            .context("Failed to extract update results")?;

        // If no nodes were updated, version mismatch occurred
        if updated_nodes.is_empty() {
            return Ok(None);
        }

        // Update spoke table if properties were provided and node type has spoke table
        if let Some(props) = updated_properties {
            if self.has_spoke_table(&updated_node_type) {
                // UPSERT with MERGE to preserve existing spoke data
                self.db
                    .query("UPSERT type::thing($table, $id) MERGE $properties;")
                    .bind(("table", updated_node_type.clone()))
                    .bind(("id", id.to_string()))
                    .bind(("properties", props))
                    .await
                    .context("Failed to upsert properties in spoke table")?;

                // Ensure data link exists
                self.db
                    .query(
                        "UPDATE type::thing('node', $id) SET data = type::thing($type_table, $id);",
                    )
                    .bind(("id", id.to_string()))
                    .bind(("type_table", updated_node_type))
                    .await
                    .context("Failed to set data link")?;
            }
        }

        // Fetch fresh node with hydrated properties from spoke table
        let node = self
            .get_node(id)
            .await?
            .ok_or_else(|| anyhow::anyhow!("Node not found after update"))?;

        // Notify registered callback of the store change (Issue #718)
        self.notify(StoreChange {
            operation: StoreOperation::Updated,
            node: node.clone(),
            source,
        });

        Ok(Some(node))
    }

    pub async fn delete_node(&self, id: &str, source: Option<String>) -> Result<DeleteResult> {
        // Get node to determine type for Record ID
        let node = match self.get_node(id).await? {
            Some(n) => n,
            None => return Ok(DeleteResult { existed: false }),
        };

        // Delete using record IDs and graph edges
        // Use transaction for atomicity (all or nothing)
        let transaction_query = "
            BEGIN TRANSACTION;
            DELETE type::thing($table, $id);
            DELETE type::thing('node', $id);
            DELETE mentions WHERE in = type::thing('node', $id) OR out = type::thing('node', $id);
            DELETE has_child WHERE in = type::thing('node', $id) OR out = type::thing('node', $id);
            COMMIT TRANSACTION;
        ";

        self.db
            .query(transaction_query)
            .bind(("table", node.node_type.clone()))
            .bind(("id", node.id.clone()))
            .await
            .context("Failed to delete node and relations")?;

        // Notify registered callback of the store change (Issue #718)
        // For deletes, we include the pre-deletion node state
        self.notify(StoreChange {
            operation: StoreOperation::Deleted,
            node,
            source,
        });

        Ok(DeleteResult { existed: true })
    }

    /// Delete a node with cascade cleanup in a single atomic transaction
    ///
    /// This is an enhanced atomic version of delete_node. It guarantees that either:
    /// - The node, type-specific record, all edges, and all mentions are ALL deleted
    /// - OR nothing is deleted (transaction rolls back on failure)
    ///
    /// # Cascade Cleanup
    ///
    /// Deletes the following in one atomic transaction:
    /// - Node record from universal `node` table
    /// - Type-specific record (if type has properties table)
    /// - All incoming and outgoing `has_child` edges
    /// - All incoming and outgoing `mentions` edges
    ///
    /// # Arguments
    ///
    /// * `node_id` - ID of the node to delete
    ///
    /// # Returns
    ///
    /// DeleteResult indicating whether the node existed
    ///
    /// # Example
    ///
    /// ```rust,no_run
    /// # use nodespace_core::db::SurrealStore;
    /// # async fn example(store: &SurrealStore) -> anyhow::Result<()> {
    /// let result = store.delete_node_cascade_atomic("node-uuid", None).await?;
    /// assert!(result.existed);
    /// # Ok(())
    /// # }
    /// ```
    pub async fn delete_node_cascade_atomic(
        &self,
        node_id: &str,
        source: Option<String>,
    ) -> Result<DeleteResult> {
        // Get node to determine type for Record ID
        let node = match self.get_node(node_id).await? {
            Some(n) => n,
            None => return Ok(DeleteResult { existed: false }),
        };

        // Build atomic cascade delete transaction using Thing parameters
        // This ensures ALL related data is deleted or NOTHING is deleted
        let node_type = node.node_type.clone();
        let node_id_str = node.id.clone();

        let transaction_query = r#"
            BEGIN TRANSACTION;

            -- Delete type-specific record (if exists)
            DELETE $type_id;

            -- Delete node from universal table
            DELETE $node_id;

            -- Delete all has_child edges (incoming and outgoing)
            DELETE has_child WHERE in = $node_id OR out = $node_id;

            -- Delete all mention edges (incoming and outgoing)
            DELETE mentions WHERE in = $node_id OR out = $node_id;

            COMMIT TRANSACTION;
        "#
        .to_string();

        // Construct Thing objects for Record IDs
        let node_thing = surrealdb::sql::Thing::from(("node".to_string(), node_id_str.clone()));
        let type_thing = surrealdb::sql::Thing::from((node_type.clone(), node_id_str.clone()));

        // Execute transaction (we don't care about the return value, just the side effects)
        self.db
            .query(&transaction_query)
            .bind(("node_id", node_thing))
            .bind(("type_id", type_thing))
            .await
            .map(|_| ())
            .context(format!(
                "Failed to delete node '{}' (type: {}) with cascade",
                node_id_str, node_type
            ))?;

        // Notify registered callback of the store change (Issue #718)
        // For deletes, we include the pre-deletion node state
        self.notify(StoreChange {
            operation: StoreOperation::Deleted,
            node,
            source,
        });

        Ok(DeleteResult { existed: true })
    }

    /// Delete a node with version check (optimistic locking)
    ///
    /// Only deletes the node if its version matches the expected version.
    /// Returns the number of rows affected (0 if version mismatch, 1 if deleted).
    pub async fn delete_with_version_check(
        &self,
        id: &str,
        expected_version: i64,
        source: Option<String>,
    ) -> Result<usize> {
        // First get the node to check version
        let node = match self.get_node(id).await? {
            Some(n) => n,
            None => return Ok(0), // Node doesn't exist
        };

        // Check version match
        if node.version != expected_version {
            return Ok(0); // Version mismatch, no deletion
        }

        // Version matches, proceed with deletion
        // Note: delete_node handles the notification
        let result = self.delete_node(id, source).await?;
        Ok(if result.existed { 1 } else { 0 })
    }

    pub async fn query_nodes(&self, query: NodeQuery) -> Result<Vec<Node>> {
        // Handle mentioned_by query using graph traversal
        // See: docs/architecture/data/surrealdb-schema-design.md - Graph Traversal Patterns
        if let Some(ref mentioned_node_id) = query.mentioned_by {
            // Use graph traversal to get IDs, then fetch full nodes
            // We can't use SELECT <-mentions<-node.* directly because it returns nested structure
            let sql = if query.limit.is_some() {
                "SELECT VALUE <-mentions<-node.id FROM type::thing('node', $node_id) LIMIT $limit;"
            } else {
                "SELECT VALUE <-mentions<-node.id FROM type::thing('node', $node_id);"
            };

            let mut query_builder = self
                .db
                .query(sql)
                .bind(("node_id", mentioned_node_id.to_string()));

            if let Some(limit) = query.limit {
                query_builder = query_builder.bind(("limit", limit));
            }

            let mut response = query_builder
                .await
                .context("Failed to query mentioned_by nodes")?;

            // SELECT VALUE with graph traversal returns nested array - flatten it
            // Result format: [[thing1, thing2], [thing3]] from multiple source nodes
            let source_things_nested: Vec<Vec<Thing>> = response
                .take(0)
                .context("Failed to extract source node IDs from mentions")?;

            let source_things: Vec<Thing> = source_things_nested.into_iter().flatten().collect();

            // Extract UUIDs and fetch full node records
            let mut nodes = Vec::new();
            for thing in source_things {
                if let Id::String(id_str) = &thing.id {
                    // id_str is just the UUID
                    if let Some(node) = self.get_node(id_str).await? {
                        nodes.push(node);
                    }
                }
            }

            return Ok(nodes);
        }

        // Handle content_contains query
        if let Some(ref search_query) = query.content_contains {
            let nodes = self
                .search_nodes_by_content(search_query, query.limit.map(|l| l as i64))
                .await?;

            // Note: Filtering by root/task status is done at the Tauri command layer
            // (mention_autocomplete), not here, since it requires graph traversal.
            return Ok(nodes);
        }

        // Build WHERE clause conditions
        let mut conditions = Vec::new();

        if query.node_type.is_some() {
            conditions.push("node_type = $node_type".to_string());
        }

        // Note: Filtering for mentionable nodes (roots + tasks) is done in mention_autocomplete command

        // Build SQL query
        let where_clause = if !conditions.is_empty() {
            Some(conditions.join(" AND "))
        } else {
            None
        };

        let sql = match (&where_clause, query.limit) {
            (None, None) => "SELECT * FROM node;".to_string(),
            (None, Some(_)) => "SELECT * FROM node LIMIT $limit;".to_string(),
            (Some(clause), None) => format!("SELECT * FROM node WHERE {};", clause),
            (Some(clause), Some(_)) => {
                format!("SELECT * FROM node WHERE {} LIMIT $limit;", clause)
            }
        };

        let mut query_builder = self.db.query(sql);

        if let Some(node_type) = &query.node_type {
            query_builder = query_builder.bind(("node_type", node_type.clone()));
        }

        if let Some(limit) = query.limit {
            query_builder = query_builder.bind(("limit", limit));
        }

        let mut response = query_builder.await.context("Failed to query nodes")?;
        let surreal_nodes: Vec<SurrealNode> = response
            .take(0)
            .context("Failed to extract nodes from query response")?;

        // Convert to nodes
        let mut nodes: Vec<Node> = surreal_nodes.into_iter().map(Into::into).collect();

        // Batch fetch properties for nodes that have them (same as get_children)
        // Performance: 100 nodes with properties = 2-3 queries (vs 101 with N+1 pattern)
        use std::collections::HashMap;

        // Group nodes by type for batch fetching
        let mut nodes_by_type: HashMap<String, Vec<String>> = HashMap::new();
        for node in &nodes {
            if self.has_spoke_table(&node.node_type) {
                nodes_by_type
                    .entry(node.node_type.clone())
                    .or_default()
                    .push(node.id.clone());
            }
        }

        // Batch fetch properties for each type
        let mut all_properties: HashMap<String, Value> = HashMap::new();
        for (node_type, node_ids) in nodes_by_type {
            match batch_fetch_properties(&self.db, &node_type, &node_ids).await {
                Ok(props_map) => {
                    all_properties.extend(props_map);
                }
                Err(e) => {
                    tracing::warn!(
                        "Failed to batch fetch properties for type '{}': {}",
                        node_type,
                        e
                    );
                }
            }
        }

        // Hydrate properties into nodes
        for node in &mut nodes {
            if let Some(props) = all_properties.get(&node.id) {
                node.properties = props.clone();
            }
        }

        Ok(nodes)
    }

    pub async fn get_children(&self, parent_id: Option<&str>) -> Result<Vec<Node>> {
        // Use graph edges for hierarchy traversal with fractional ordering (Issue #550)
        // Note: We don't use FETCH data because it causes deserialization issues (same as get_node)
        let surreal_nodes = if let Some(parent_id) = parent_id {
            // Create Thing record ID for parent node
            use surrealdb::sql::Thing;
            let parent_thing = Thing::from(("node".to_string(), parent_id.to_string()));

            // Query children ordered by edge.order (fractional ordering)
            // Build ordered list of child IDs, then fetch full nodes in that order
            let mut edge_response = self
                .db
                .query("SELECT * FROM has_child WHERE in = $parent_thing ORDER BY order ASC;")
                .bind(("parent_thing", parent_thing.clone()))
                .await
                .context("Failed to get child edges")?;

            #[derive(serde::Deserialize)]
            struct EdgeOut {
                out: Thing,
            }

            let edges: Vec<EdgeOut> = edge_response
                .take(0)
                .context("Failed to extract child edges")?;

            // Extract node Things in order
            let mut ordered_node_things: Vec<Thing> = Vec::new();
            let mut ordered_node_strs: Vec<String> = Vec::new();
            for edge in edges {
                ordered_node_things.push(edge.out.clone());
                ordered_node_strs.push(format!("{}", edge.out));
            }

            // Fetch all nodes using Things for proper ID matching
            let mut response = self
                .db
                .query("SELECT * FROM node WHERE id IN $ids;")
                .bind(("ids", ordered_node_things))
                .await
                .context("Failed to get children")?;

            let fetched_nodes: Vec<SurrealNode> = response
                .take(0)
                .context("Failed to extract children from response")?;

            // Create a hashmap of fetched nodes for quick lookup by ID
            use std::collections::HashMap as IDHashMap;
            let mut node_map: IDHashMap<String, SurrealNode> = IDHashMap::new();
            for node in fetched_nodes {
                let id_str = format!("{}", node.id);
                node_map.insert(id_str, node);
            }

            // Reconstruct nodes in the exact order from the edge query (ordered_node_strs)
            let mut nodes: Vec<SurrealNode> = Vec::new();
            for id_str in ordered_node_strs.iter() {
                if let Some(node) = node_map.remove(id_str) {
                    nodes.push(node);
                }
            }

            nodes
        } else {
            // Root nodes: nodes that have NO incoming has_child edges
            let mut response = self
                .db
                .query("SELECT * FROM node WHERE count(<-has_child) = 0;")
                .await
                .context("Failed to get root nodes")?;

            let nodes: Vec<SurrealNode> = response
                .take(0)
                .context("Failed to extract root nodes from response")?;

            nodes
        };

        // Convert to nodes
        let mut nodes: Vec<Node> = surreal_nodes.into_iter().map(Into::into).collect();

        // Batch fetch properties for nodes that have them
        // Performance: 100 nodes with properties = 2-3 queries (vs 101 with N+1 pattern)
        use std::collections::HashMap;

        // Group nodes by type for batch fetching
        let mut nodes_by_type: HashMap<String, Vec<String>> = HashMap::new();
        for node in &nodes {
            if self.has_spoke_table(&node.node_type) {
                nodes_by_type
                    .entry(node.node_type.clone())
                    .or_default()
                    .push(node.id.clone());
            }
        }

        // Batch fetch properties for each type
        let mut all_properties: HashMap<String, Value> = HashMap::new();
        for (node_type, node_ids) in nodes_by_type {
            match batch_fetch_properties(&self.db, &node_type, &node_ids).await {
                Ok(props_map) => {
                    all_properties.extend(props_map);
                }
                Err(e) => {
                    tracing::warn!(
                        "Failed to batch fetch properties for type '{}': {}",
                        node_type,
                        e
                    );
                }
            }
        }

        // Hydrate properties into nodes
        for node in &mut nodes {
            if let Some(props) = all_properties.get(&node.id) {
                node.properties = props.clone();
            }
        }

        // Children are ordered by has_child edge order field (fractional ordering)
        // Sorting is done by the database query ORDER BY clause
        Ok(nodes)
    }

    /// Get the parent of a node (via incoming has_child edge)
    ///
    /// Returns the node's parent if it has one, or None if it's a root node.
    ///
    /// # Arguments
    ///
    /// * `child_id` - The child node ID
    ///
    /// # Returns
    ///
    /// `Some(parent_node)` if the node has a parent, `None` if it's a root node
    pub async fn get_parent(&self, child_id: &str) -> Result<Option<Node>> {
        use surrealdb::sql::Thing;
        let child_thing = Thing::from(("node".to_string(), child_id.to_string()));

        // Query for parent via incoming has_child edge
        // SELECT * FROM node WHERE id IN (SELECT VALUE in FROM has_child WHERE out = $child_thing)
        let mut response = self
            .db
            .query("SELECT * FROM node WHERE id IN (SELECT VALUE in FROM has_child WHERE out = $child_thing) LIMIT 1;")
            .bind(("child_thing", child_thing))
            .await
            .context("Failed to get parent")?;

        let nodes: Vec<SurrealNode> = response
            .take(0)
            .context("Failed to extract parent from response")?;

        if nodes.is_empty() {
            return Ok(None);
        }

        // Convert to node and fetch properties if needed
        let mut node: Node = nodes.into_iter().next().unwrap().into();

        // Fetch properties if this node type has them
        if self.has_spoke_table(&node.node_type) {
            if let Ok(props_map) =
                batch_fetch_properties(&self.db, &node.node_type, &[node.id.clone()]).await
            {
                if let Some(props) = props_map.get(&node.id) {
                    node.properties = props.clone();
                }
            }
        }

        Ok(Some(node))
    }

    /// Get entire node tree recursively in a SINGLE query
    ///
    /// This method leverages SurrealDB's recursive graph traversal to fetch
    /// a node and ALL its descendants at all levels in one database query.
    ///
    /// # Performance
    ///
    /// - **1 query** regardless of tree depth/size (vs N queries for manual traversal)
    /// - Ideal for: outline view, export, tree visualization
    ///
    /// # Arguments
    ///
    /// * `root_id` - ID of the root node to fetch tree from
    ///
    /// # Returns
    ///
    /// Returns root node with nested `children` arrays at all levels.
    /// Each node includes properties fetched from type-specific tables.
    ///
    /// # Example
    ///
    /// ```text
    /// // Get entire tree in ONE query:
    /// let tree = store.get_node_tree("root-uuid").await?;
    /// // tree.children[0].children[0]... (fully nested)
    /// ```
    ///
    /// # Implementation Note
    ///
    /// Uses SurrealDB's `@` recursive projection operator:
    /// ```sql
    /// SELECT id, title,
    ///   ->has_child->node.{
    ///     id, title,
    ///     children: ->has_child->node.@  -- '@' repeats this projection recursively
    ///   } AS children
    /// FROM node:root_uuid;
    /// ```
    pub async fn get_node_tree(&self, root_id: &str) -> Result<Option<serde_json::Value>> {
        // NOTE: SurrealDB's `@` recursive repeat operator is NOT supported with RocksDB storage
        // (returns UnsupportedRepeatRecurse error). We use Rust recursion instead.
        //
        // This fetches the tree by:
        // 1. Getting the root node
        // 2. Recursively fetching children using get_children()
        // 3. Building nested JSON structure

        // First, get the root node
        let root_node = match self.get_node(root_id).await? {
            Some(node) => node,
            None => return Ok(None),
        };

        // Build nested tree recursively
        let tree = self.build_node_tree_recursive(&root_node).await?;
        Ok(Some(tree))
    }

    /// Recursively build a node tree as JSON with nested children
    ///
    /// # Safety Guards
    /// - Maximum depth limit (100 levels) prevents stack overflow on deeply nested hierarchies
    /// - Cycle detection prevents infinite recursion on cyclic graphs
    fn build_node_tree_recursive<'a>(
        &'a self,
        node: &'a Node,
    ) -> std::pin::Pin<Box<dyn std::future::Future<Output = Result<serde_json::Value>> + Send + 'a>>
    {
        const MAX_DEPTH: usize = 100;

        Box::pin(async move {
            self.build_node_tree_with_guards(
                node,
                0,
                MAX_DEPTH,
                &mut std::collections::HashSet::new(),
            )
            .await
        })
    }

    /// Internal implementation with depth tracking and cycle detection
    fn build_node_tree_with_guards<'a>(
        &'a self,
        node: &'a Node,
        depth: usize,
        max_depth: usize,
        visited: &'a mut std::collections::HashSet<String>,
    ) -> std::pin::Pin<Box<dyn std::future::Future<Output = Result<serde_json::Value>> + Send + 'a>>
    {
        Box::pin(async move {
            // Depth limit check - prevent stack overflow
            if depth >= max_depth {
                return Err(anyhow::anyhow!(
                "Maximum tree depth ({}) exceeded at node '{}'. This may indicate a very deep hierarchy or a cycle.",
                max_depth,
                node.id
            ));
            }

            // Cycle detection - prevent infinite recursion
            if visited.contains(&node.id) {
                return Err(anyhow::anyhow!(
                    "Cycle detected: node '{}' appears multiple times in the hierarchy path",
                    node.id
                ));
            }
            visited.insert(node.id.clone());

            // Get ordered children for this node (get_children returns Vec<Node> already ordered)
            let children_nodes = self.get_children(Some(&node.id)).await?;

            // Recursively build children trees
            let mut children_json = Vec::new();
            for child_node in &children_nodes {
                let child_tree = self
                    .build_node_tree_with_guards(child_node, depth + 1, max_depth, visited)
                    .await?;
                children_json.push(child_tree);
            }

            // Backtrack: remove from visited set to allow node to appear in other branches
            visited.remove(&node.id);

            // Build JSON for this node with children
            // NOTE: Use bare node.id without "node:" prefix to match frontend expectations
            Ok(serde_json::json!({
                "id": node.id,
                "type": node.node_type,
                "content": node.content,
                "version": node.version,
                "created_at": node.created_at,
                "modified_at": node.modified_at,
                "mentions": node.mentions,
                "mentioned_by": node.mentioned_by,
                "data": node.properties,
                "variants": serde_json::Value::Null,
                "_schema_version": 1,
                "children": children_json
            }))
        })
    }

    /// Get all nodes in a subtree using breadth-first traversal
    ///
    /// Fetches all nodes that are descendants of the given root node (not including the root itself).
    /// This is the first step in building an adjacency list structure for efficient tree navigation.
    ///
    /// # Query Strategy
    ///
    /// Uses iterative breadth-first traversal: queries each level's children until no more
    /// children are found. This approach is more compatible across SurrealDB configurations
    /// than recursive syntax.
    ///
    /// # Arguments
    ///
    /// * `root_id` - ID of the root node to fetch descendants for
    ///
    /// # Returns
    ///
    /// Vector of all descendant nodes (flat, in breadth-first order)
    ///
    /// # Performance
    ///
    /// O(depth) queries where depth is the tree depth. Each level is fetched in 2 queries
    /// (one for edges, one for node data).
    pub async fn get_nodes_in_subtree(&self, root_id: &str) -> Result<Vec<Node>> {
        use surrealdb::sql::Thing;

        // Strategy: Iterative breadth-first traversal using edge queries
        // This avoids SurrealDB's recursive syntax which has compatibility issues
        let mut all_descendants = Vec::new();
        let mut current_level = vec![root_id.to_string()];

        // Maximum depth to prevent infinite loops (256 is SurrealDB's max recursion depth)
        const MAX_DEPTH: usize = 256;

        for _ in 0..MAX_DEPTH {
            if current_level.is_empty() {
                break;
            }

            // Build query for all children of current level nodes
            // Use type::thing() to properly construct record IDs
            let parent_ids: Vec<String> = current_level
                .iter()
                .map(|id| format!("type::thing('node', '{}')", id))
                .collect();

            let query = format!(
                "SELECT VALUE out FROM has_child WHERE in IN [{}];",
                parent_ids.join(", ")
            );

            let mut response = self
                .db
                .query(&query)
                .await
                .context("Failed to query children")?;

            let child_things: Vec<Thing> =
                response.take(0).context("Failed to extract child IDs")?;

            if child_things.is_empty() {
                break;
            }

            // Extract string IDs for next level
            let child_ids: Vec<String> = child_things
                .iter()
                .map(|t| match &t.id {
                    Id::String(s) => s.clone(),
                    Id::Number(n) => n.to_string(),
                    _ => t.id.to_string(),
                })
                .collect();

            // Fetch full node data for these children
            // Use type::thing() to properly construct record IDs (UUIDs need proper quoting)
            let child_id_list: Vec<String> = child_ids
                .iter()
                .map(|id| format!("type::thing('node', '{}')", id))
                .collect();

            let nodes_query = format!(
                "SELECT * FROM node WHERE id IN [{}] FETCH data;",
                child_id_list.join(", ")
            );

            let mut nodes_response = self
                .db
                .query(&nodes_query)
                .await
                .context("Failed to fetch child nodes")?;

            let surreal_nodes: Vec<SurrealNode> = nodes_response
                .take(0)
                .context("Failed to extract child nodes")?;

            all_descendants.extend(surreal_nodes.into_iter().map(Into::into));

            // Move to next level
            current_level = child_ids;
        }

        // Enrich nodes with spoke data for types that have spoke tables
        // This follows the same pattern as get_node() which uses a two-query approach
        // to properly fetch spoke data for task/schema nodes
        let enriched_nodes = self.enrich_nodes_with_spoke_data(all_descendants).await?;

        Ok(enriched_nodes)
    }

    /// Enrich nodes with spoke table data for types that have dedicated spoke tables
    ///
    /// For node types like "task" and "schema" that store their type-specific data
    /// in separate spoke tables, this function batch-fetches the spoke data and
    /// merges it into the node's properties field.
    ///
    /// This is necessary because the FETCH data query doesn't properly populate
    /// the data field due to SurrealDB's Thing type serialization issues.
    async fn enrich_nodes_with_spoke_data(&self, mut nodes: Vec<Node>) -> Result<Vec<Node>> {
        use std::collections::HashMap;

        // Group task node IDs for batch fetching
        let task_ids: Vec<&str> = nodes
            .iter()
            .filter(|n| n.node_type == "task")
            .map(|n| n.id.as_str())
            .collect();

        if task_ids.is_empty() {
            return Ok(nodes);
        }

        // Batch fetch task spoke data
        let task_id_list: Vec<String> = task_ids
            .iter()
            .map(|id| format!("type::thing('task', '{}')", id))
            .collect();

        let spoke_query = format!(
            "SELECT * OMIT id, node FROM task WHERE id IN [{}];",
            task_id_list.join(", ")
        );

        let mut spoke_response = self
            .db
            .query(&spoke_query)
            .await
            .context("Failed to fetch task spoke data")?;

        let spoke_results: Vec<Value> = spoke_response.take(0).unwrap_or_default();

        // Build a map of node_id -> spoke properties
        // The spoke query returns records where we need to extract the node ID from context
        // Since we used the same order, we can match by position
        let mut spoke_map: HashMap<String, Value> = HashMap::new();
        for (i, spoke_data) in spoke_results.into_iter().enumerate() {
            if let Some(id) = task_ids.get(i) {
                spoke_map.insert(id.to_string(), spoke_data);
            }
        }

        // Merge spoke data into node properties
        for node in nodes.iter_mut() {
            if node.node_type == "task" {
                if let Some(spoke_data) = spoke_map.remove(&node.id) {
                    // Replace properties with spoke data (which contains status, priority, etc.)
                    node.properties = spoke_data;
                }
            }
        }

        Ok(nodes)
    }

    /// Get all edges in a subtree using adjacency list strategy
    ///
    /// Fetches all parent-child relationships (has_child edges) within a subtree.
    /// Combined with `get_nodes_in_subtree()`, this enables building an in-memory adjacency list
    /// for efficient tree construction and navigation.
    ///
    /// # Strategy
    ///
    /// First gets all descendant node IDs using `get_nodes_in_subtree()`, then queries
    /// all edges where the parent is either the root or a descendant.
    ///
    /// # Arguments
    ///
    /// * `root_id` - ID of the root node to fetch descendant edges for
    ///
    /// # Returns
    ///
    /// Vector of all edges within the subtree (parent-child relationships)
    pub async fn get_edges_in_subtree(&self, root_id: &str) -> Result<Vec<EdgeRecord>> {
        use surrealdb::sql::Thing;

        let root_thing = Thing::from(("node".to_string(), root_id.to_string()));

        // Get all descendant nodes first
        let descendants = self.get_nodes_in_subtree(root_id).await?;

        // Build a list of all node IDs (root + descendants) for the WHERE clause
        let all_node_ids: Vec<String> = descendants.iter().map(|n| n.id.clone()).collect();

        // Query edges where parent is either root or a descendant
        // If no descendants, just query edges from root
        let query = if all_node_ids.is_empty() {
            "SELECT id, in, out, order FROM has_child WHERE in = $root_thing ORDER BY order ASC;"
                .to_string()
        } else {
            // Include root ID in the list
            // Use type::thing() to properly construct record IDs (UUIDs need proper quoting)
            let id_list = std::iter::once(format!("type::thing('node', '{}')", root_id))
                .chain(
                    all_node_ids
                        .iter()
                        .map(|id| format!("type::thing('node', '{}')", id)),
                )
                .collect::<Vec<_>>()
                .join(", ");

            format!(
                "SELECT id, in, out, order FROM has_child WHERE in IN [{}] ORDER BY order ASC;",
                id_list
            )
        };

        let mut response = self
            .db
            .query(&query)
            .bind(("root_thing", root_thing.clone()))
            .await
            .context("Failed to fetch subtree edges")?;

        // Use Thing type for SurrealDB record IDs
        #[derive(serde::Deserialize)]
        struct EdgeRow {
            id: Thing,
            #[serde(rename = "in")]
            in_node: Thing,
            #[serde(rename = "out")]
            out_node: Thing,
            order: f64,
        }

        let edges: Vec<EdgeRow> = response
            .take(0)
            .context("Failed to extract subtree edges from response")?;

        // Extract string IDs from Thing types
        Ok(edges
            .into_iter()
            .map(|e| {
                let id_str = match &e.id.id {
                    Id::String(s) => s.clone(),
                    Id::Number(n) => n.to_string(),
                    _ => e.id.to_string(),
                };
                let in_str = match &e.in_node.id {
                    Id::String(s) => s.clone(),
                    Id::Number(n) => n.to_string(),
                    _ => e.in_node.to_string(),
                };
                let out_str = match &e.out_node.id {
                    Id::String(s) => s.clone(),
                    Id::Number(n) => n.to_string(),
                    _ => e.out_node.to_string(),
                };
                EdgeRecord {
                    id: id_str,
                    in_node: in_str,
                    out_node: out_str,
                    order: e.order,
                }
            })
            .collect())
    }

    pub async fn search_nodes_by_content(
        &self,
        search_query: &str,
        limit: Option<i64>,
    ) -> Result<Vec<Node>> {
        // Use string::lowercase() for case-insensitive search
        // SurrealDB CONTAINS is case-sensitive by default
        let sql = if limit.is_some() {
            "SELECT * FROM node WHERE string::lowercase(content) CONTAINS string::lowercase($search_query) LIMIT $limit;"
        } else {
            "SELECT * FROM node WHERE string::lowercase(content) CONTAINS string::lowercase($search_query);"
        };

        let mut query_builder = self
            .db
            .query(sql)
            .bind(("search_query", search_query.to_string()));

        if let Some(lim) = limit {
            query_builder = query_builder.bind(("limit", lim));
        }

        let mut response = query_builder.await.context("Failed to search nodes")?;
        let surreal_nodes: Vec<SurrealNode> = response
            .take(0)
            .context("Failed to extract search results from response")?;
        Ok(surreal_nodes.into_iter().map(Into::into).collect())
    }

    /// Search nodes for mention autocomplete with proper filtering
    ///
    /// Applies mention-specific filtering rules:
    /// - Excludes: date, schema node types (always)
    /// - Text-based types (text, header, code-block, quote-block, ordered-list): only root nodes
    /// - Other types (task, query, etc.): included regardless of hierarchy
    ///
    /// # Arguments
    ///
    /// * `search_query` - Content search string (case-insensitive)
    /// * `limit` - Maximum number of results
    ///
    /// # Returns
    ///
    /// Filtered nodes matching mention autocomplete criteria
    pub async fn mention_autocomplete(
        &self,
        search_query: &str,
        limit: Option<i64>,
    ) -> Result<Vec<Node>> {
        // Node types that are always excluded from mention autocomplete
        const EXCLUDED_TYPES: &[&str] = &["date", "schema"];

        // Text-based types that must be root nodes (no parent) to be included
        const TEXT_TYPES: &[&str] = &[
            "text",
            "header",
            "code-block",
            "quote-block",
            "ordered-list",
        ];

        // Build SQL with filtering logic:
        // 1. Content search (case-insensitive)
        // 2. Exclude date and schema types
        // 3. For text types: only include if root (count(<-has_child) = 0)
        // 4. For other types: include regardless of hierarchy
        let sql = r#"
            SELECT * FROM node
            WHERE string::lowercase(content) CONTAINS string::lowercase($search_query)
              AND node_type NOT IN $excluded_types
              AND (
                node_type NOT IN $text_types
                OR
                count(<-has_child) = 0
              )
            LIMIT $limit;
        "#;

        let effective_limit = limit.unwrap_or(10);

        let mut response = self
            .db
            .query(sql)
            .bind(("search_query", search_query.to_string()))
            .bind(("excluded_types", EXCLUDED_TYPES.to_vec()))
            .bind(("text_types", TEXT_TYPES.to_vec()))
            .bind(("limit", effective_limit))
            .await
            .context("Failed to search nodes for mention autocomplete")?;

        let surreal_nodes: Vec<SurrealNode> = response
            .take(0)
            .context("Failed to extract mention autocomplete results")?;

        Ok(surreal_nodes.into_iter().map(Into::into).collect())
    }

    /// Validate that creating a parent-child relationship won't create a cycle
    ///
    /// **Purpose**: Prevents cyclic references in the node hierarchy tree.
    ///
    /// **Example Cycle**: A→B→C→A (adding A as child of C would create this)
    ///
    /// **Impact if not validated**:
    /// - Infinite loops in tree traversal queries
    /// - Stack overflow in recursive operations
    /// - Data corruption in hierarchy
    ///
    /// # Arguments
    ///
    /// * `parent_id` - Proposed parent node ID
    /// * `child_id` - Proposed child node ID
    ///
    /// # Returns
    ///
    /// `Ok(())` if no cycle would be created, `Err` if cycle detected
    ///
    /// # Examples
    ///
    /// ```text
    /// // Valid: A→B, B→C (adding C as child of B)
    /// validate_no_cycle("B", "C").await?; // ✓ OK
    ///
    /// // Invalid: A→B→C, trying to add A as child of C
    /// validate_no_cycle("C", "A").await?; // ✗ Error: would create cycle A→B→C→A
    /// ```
    async fn validate_no_cycle(&self, parent_id: &str, child_id: &str) -> Result<()> {
        use surrealdb::sql::Thing;

        // Check if parent is a descendant of child
        // If so, creating this edge would create a cycle
        let child_thing = Thing::from(("node".to_string(), child_id.to_string()));

        // Query: Get all descendants of child node recursively
        // Then check if parent is in that list
        // Using SurrealDB recursive graph traversal syntax (v2.1+) to check ALL descendant levels
        // The `{..+collect}` syntax means unbounded recursive traversal collecting unique nodes
        // This will detect cycles at any level: A→B (direct), A→B→C (3-node), A→B→C→D (4-node), etc.
        let query = "
            LET $descendants = $child_thing.{..+collect}->has_child->node;
            SELECT * FROM type::thing('node', $parent_id)
            WHERE id IN $descendants
            LIMIT 1;
        ";

        let mut response = self
            .db
            .query(query)
            .bind(("parent_id", parent_id.to_string()))
            .bind(("child_thing", child_thing))
            .await
            .context("Failed to check for cycles")?;

        // The query has 2 statements (LET, SELECT), we want the SELECT result at index 1
        let results: Vec<SurrealNode> = response
            .take(1)
            .context("Failed to parse cycle check results")?;

        if !results.is_empty() {
            return Err(anyhow::anyhow!(
                "Cannot create parent-child relationship: would create cycle. \
                Node '{}' is a descendant of node '{}', so '{}' cannot be a parent of '{}'.",
                parent_id,
                child_id,
                parent_id,
                child_id
            ));
        }

        Ok(())
    }

    /// Rebalance child ordering for a parent when precision degrades
    ///
    /// When fractional ordering gets too granular (gaps < 0.0001), this rebalances
    /// all children of a parent to have even spacing (1.0, 2.0, 3.0, etc.).
    ///
    /// This operation is atomic - either all children are rebalanced or none are.
    ///
    /// # Arguments
    ///
    /// * `parent_id` - ID of the parent node whose children should be rebalanced
    ///
    /// # Returns
    ///
    /// Ok(()) on success
    async fn rebalance_children_for_parent(&self, parent_id: &str) -> Result<()> {
        use surrealdb::sql::Thing;

        // Step 1: Get all children in current order
        let parent_thing = Thing::from(("node".to_string(), parent_id.to_string()));

        #[derive(Deserialize)]
        struct EdgeOut {
            out: Thing,
        }

        let mut edges_response = self
            .db
            .query("SELECT out FROM has_child WHERE in = $parent_thing ORDER BY order ASC;")
            .bind(("parent_thing", parent_thing.clone()))
            .await
            .context("Failed to get children for rebalancing")?;

        let edges: Vec<EdgeOut> = edges_response
            .take(0)
            .context("Failed to extract children for rebalancing")?;

        if edges.is_empty() {
            return Ok(()); // Nothing to rebalance
        }

        // Step 2: Calculate new orders [1.0, 2.0, 3.0, ...]
        let new_orders = FractionalOrderCalculator::rebalance(edges.len());

        // Step 3: Build atomic transaction to update all edges
        // We need to update each has_child edge's order field
        let mut transaction = String::from("BEGIN TRANSACTION;\n");

        for (i, _edge) in edges.iter().enumerate() {
            let new_order = new_orders[i];
            transaction.push_str(&format!(
                "UPDATE has_child SET order = {} WHERE in = $parent_thing AND out = $out{} FETCH AFTER;\n",
                new_order, i
            ));
        }

        transaction.push_str("COMMIT TRANSACTION;");

        // Step 4: Execute transaction with all edges bound
        let mut query_builder = self
            .db
            .query(&transaction)
            .bind(("parent_thing", parent_thing));

        for (i, edge) in edges.iter().enumerate() {
            query_builder = query_builder.bind((format!("out{}", i), edge.out.clone()));
        }

        query_builder
            .await
            .context("Failed to rebalance children")?;

        Ok(())
    }

    /// Move a node to a new parent atomically
    ///
    /// Guarantees that either:
    /// - The old edge is deleted AND the new edge is created
    /// - OR nothing changes (transaction rolls back on failure)
    ///
    /// # Arguments
    ///
    /// * `node_id` - ID of the node to move
    /// * `new_parent_id` - ID of the new parent (None = make root node)
    /// * `insert_after_sibling_id` - Optional sibling to insert after (uses edge-based fractional ordering)
    ///
    /// # Returns
    ///
    /// Ok(()) on success
    ///
    /// # Example
    ///
    /// ```rust,no_run
    /// # use nodespace_core::db::SurrealStore;
    /// # async fn example(store: &SurrealStore) -> anyhow::Result<()> {
    /// // Move node to new parent
    /// store.move_node("child-uuid", Some("new-parent-uuid"), None).await?;
    ///
    /// // Make node a root node
    /// store.move_node("child-uuid", None, None).await?;
    /// # Ok(())
    /// # }
    /// ```
    pub async fn move_node(
        &self,
        node_id: &str,
        new_parent_id: Option<&str>,
        insert_after_sibling_id: Option<&str>,
    ) -> Result<()> {
        // Convert parameters to owned strings for 'static lifetime
        let node_id = node_id.to_string();
        let new_parent_id = new_parent_id.map(|s| s.to_string());
        let insert_after_sibling_id = insert_after_sibling_id.map(|s| s.to_string());

        // Validate node exists
        let _node = self
            .get_node(&node_id)
            .await?
            .ok_or_else(|| anyhow::anyhow!("Node not found: {}", node_id))?;

        // Validate that moving won't create a cycle
        if let Some(ref parent_id) = new_parent_id {
            // Validate parent exists
            let parent_exists = self.get_node(parent_id).await?;
            if parent_exists.is_none() {
                return Err(anyhow::anyhow!("Parent node not found: {}", parent_id));
            }

            self.validate_no_cycle(parent_id, &node_id).await?;
        }

        // Calculate fractional order for the new position
        #[derive(Deserialize)]
        struct EdgeWithOrder {
            out: surrealdb::sql::Thing,
            order: f64,
        }

        let new_order = if let Some(ref parent_id) = new_parent_id {
            let parent_thing = surrealdb::sql::Thing::from(("node".to_string(), parent_id.clone()));

            // Get all child edges for this parent, ordered by order field
            let mut edges_response = self
                .db
                .query(
                    "SELECT out, order FROM has_child WHERE in = $parent_thing ORDER BY order ASC;",
                )
                .bind(("parent_thing", parent_thing.clone()))
                .await
                .context("Failed to get child edges")?;

            let edges: Vec<EdgeWithOrder> = edges_response
                .take(0)
                .context("Failed to extract child edges")?;

            if let Some(after_id) = insert_after_sibling_id {
                // Find the sibling we're inserting after
                let after_thing =
                    surrealdb::sql::Thing::from(("node".to_string(), after_id.clone()));
                let after_index = edges
                    .iter()
                    .position(|e| e.out == after_thing)
                    .ok_or_else(|| anyhow::anyhow!("Sibling not found: {}", after_id))?;

                // Get orders before and after insertion point
                let prev_order = edges[after_index].order;
                let next_order = edges.get(after_index + 1).map(|e| e.order);

                // Calculate new order between them
                let calculated =
                    FractionalOrderCalculator::calculate_order(Some(prev_order), next_order);

                // Check if rebalancing is needed
                if let Some(next) = next_order {
                    if (next - prev_order) < 0.0001 {
                        // Gap too small, need to rebalance before inserting
                        self.rebalance_children_for_parent(parent_id).await?;

                        // Re-query edges after rebalancing
                        // NOTE: There is a small race condition window here - between rebalancing
                        // completion and this re-query, another client could move/delete the sibling.
                        // If this occurs, we'll get "Sibling not found after rebalancing" error and
                        // the operation fails. This is an accepted limitation - clients can retry.
                        // A fully atomic solution would require SurrealDB to support multi-step
                        // transactions with deferred constraint checking, which isn't available.
                        let mut edges_response = self
                            .db
                            .query("SELECT out, order FROM has_child WHERE in = $parent_thing ORDER BY order ASC;")
                            .bind(("parent_thing", parent_thing.clone()))
                            .await
                            .context("Failed to get child edges after rebalancing")?;

                        let edges: Vec<EdgeWithOrder> = edges_response
                            .take(0)
                            .context("Failed to extract child edges after rebalancing")?;

                        let after_index = edges
                            .iter()
                            .position(|e| e.out == after_thing)
                            .ok_or_else(|| {
                                anyhow::anyhow!("Sibling not found after rebalancing: {}", after_id)
                            })?;

                        let prev_order = edges[after_index].order;
                        let next_order = edges.get(after_index + 1).map(|e| e.order);
                        FractionalOrderCalculator::calculate_order(Some(prev_order), next_order)
                    } else {
                        calculated
                    }
                } else {
                    calculated
                }
            } else {
                // No insert_after_sibling specified, insert at beginning
                let first_order = edges.first().map(|e| e.order);
                FractionalOrderCalculator::calculate_order(None, first_order)
            }
        } else {
            0.0 // Root nodes don't use order
        };

        // Build atomic transaction query using Thing parameters
        let transaction_query = if new_parent_id.is_some() {
            // Move to new parent
            r#"
                BEGIN TRANSACTION;

                -- Delete old parent edge
                DELETE has_child WHERE out = $node_id;

                -- Create new parent edge with fractional order
                RELATE $parent_id->has_child->$node_id CONTENT {
                    order: $order,
                    created_at: time::now()
                };

                COMMIT TRANSACTION;
            "#
            .to_string()
        } else {
            // Make root node (delete parent edge only)
            r#"
                BEGIN TRANSACTION;

                -- Delete old parent edge
                DELETE has_child WHERE out = $node_id;

                COMMIT TRANSACTION;
            "#
            .to_string()
        };

        // Construct Thing objects for Record IDs
        let node_thing = surrealdb::sql::Thing::from(("node".to_string(), node_id.clone()));
        let parent_thing = new_parent_id
            .as_ref()
            .map(|pid| surrealdb::sql::Thing::from(("node".to_string(), pid.clone())));

        // Execute transaction
        let mut query_builder = self
            .db
            .query(&transaction_query)
            .bind(("node_id", node_thing));

        if let Some(parent_thing) = parent_thing {
            query_builder = query_builder.bind(("parent_id", parent_thing));
        }

        query_builder
            .bind(("order", new_order))
            .await
            .context(format!(
                "Failed to move node '{}' to parent '{:?}'",
                node_id, new_parent_id
            ))?;

        // Note: Domain events are now emitted at NodeService layer for client filtering

        Ok(())
    }

    pub async fn create_mention(
        &self,
        source_id: &str,
        target_id: &str,
        root_id: &str,
    ) -> Result<()> {
        // Mentions relate nodes in the hub table, so we reference the node table directly
        let source_thing = surrealdb::sql::Thing::from(("node".to_string(), source_id.to_string()));
        let target_thing = surrealdb::sql::Thing::from(("node".to_string(), target_id.to_string()));

        // Check if mention already exists (for idempotency)
        let check_query = "SELECT VALUE id FROM mentions WHERE in = $source AND out = $target;";
        let mut check_response = self
            .db
            .query(check_query)
            .bind(("source", source_thing.clone()))
            .bind(("target", target_thing.clone()))
            .await
            .context("Failed to check for existing mention")?;

        let existing_mention_ids: Vec<Thing> = check_response
            .take(0)
            .context("Failed to extract mention check results")?;

        // Only create mention if it doesn't exist
        if existing_mention_ids.is_empty() {
            // TECH DEBT: SurrealDB Binding Limitation
            // ----------------------------------------
            // SurrealDB's RELATE statement does not support parameter binding for SET field values.
            // We must embed root_id directly in the query string.
            //
            // Security mitigation: root_id is a system-generated UUID from Node.id, not user input.
            // The single-quote escaping is a defense-in-depth measure, but this should be refactored
            // to use parameterized queries if SurrealDB adds support for SET bindings in RELATE.
            //
            // See: https://surrealdb.com/docs/surrealql/statements/relate
            let query = format!(
                "RELATE $source->mentions->$target SET root_id = '{}';",
                root_id.replace('\'', "''")
            );

            self.db
                .query(&query)
                .bind(("source", source_thing))
                .bind(("target", target_thing))
                .await
                .context("Failed to create mention")?;

            // Note: Domain events are now emitted at NodeService layer for client filtering
        }

        Ok(())
    }

    pub async fn delete_mention(&self, source_id: &str, target_id: &str) -> Result<()> {
        // Mentions relate nodes in the hub table, so we reference the node table directly
        let source_thing = surrealdb::sql::Thing::from(("node".to_string(), source_id.to_string()));
        let target_thing = surrealdb::sql::Thing::from(("node".to_string(), target_id.to_string()));

        self.db
            .query("DELETE FROM mentions WHERE in = $source AND out = $target;")
            .bind(("source", source_thing))
            .bind(("target", target_thing))
            .await
            .context("Failed to delete mention")?;

        // Note: Domain events are now emitted at NodeService layer for client filtering

        Ok(())
    }

    pub async fn get_outgoing_mentions(&self, node_id: &str) -> Result<Vec<String>> {
        // Use SurrealDB graph traversal syntax for optimal performance
        // See: docs/architecture/data/surrealdb-schema-design.md - Graph Traversal Patterns
        // Returns array<record> which we need to extract IDs from
        let query =
            "SELECT ->mentions->node.id AS mentioned_ids FROM type::thing('node', $node_id);";

        let mut response = self
            .db
            .query(query)
            .bind(("node_id", node_id.to_string()))
            .await
            .context("Failed to get outgoing mentions")?;

        #[derive(Debug, Deserialize)]
        struct MentionResult {
            mentioned_ids: Vec<Thing>,
        }

        // Graph traversal returns object with mentioned_ids array
        let results: Vec<MentionResult> = response
            .take(0)
            .context("Failed to extract outgoing mentions from response")?;

        // Extract UUIDs from Thing Record IDs (format: node:uuid -> uuid)
        let mentioned_ids: Vec<String> = results
            .into_iter()
            .flat_map(|r| r.mentioned_ids)
            .filter_map(|thing| {
                if let Id::String(id_str) = &thing.id {
                    // id_str is just the UUID part
                    Some(id_str.clone())
                } else {
                    None
                }
            })
            .collect();

        Ok(mentioned_ids)
    }

    pub async fn get_incoming_mentions(&self, node_id: &str) -> Result<Vec<String>> {
        // Use SurrealDB graph traversal syntax for backlinks (reverse lookup)
        // See: docs/architecture/data/surrealdb-schema-design.md - Graph Traversal Patterns
        // Returns array<record> which we need to extract IDs from
        let query =
            "SELECT <-mentions<-node.id AS mentioned_by_ids FROM type::thing('node', $node_id);";

        let mut response = self
            .db
            .query(query)
            .bind(("node_id", node_id.to_string()))
            .await
            .context("Failed to get incoming mentions")?;

        #[derive(Debug, Deserialize)]
        struct MentionResult {
            mentioned_by_ids: Vec<Thing>,
        }

        // Graph traversal returns object with mentioned_by_ids array
        let results: Vec<MentionResult> = response
            .take(0)
            .context("Failed to extract incoming mentions from response")?;

        // Extract UUIDs from Thing Record IDs (format: node:uuid -> uuid)
        let mentioned_by_ids: Vec<String> = results
            .into_iter()
            .flat_map(|r| r.mentioned_by_ids)
            .filter_map(|thing| {
                if let Id::String(id_str) = &thing.id {
                    // id_str is just the UUID part
                    Some(id_str.clone())
                } else {
                    None
                }
            })
            .collect();

        Ok(mentioned_by_ids)
    }

    pub async fn get_mentioning_containers(&self, node_id: &str) -> Result<Vec<Node>> {
        // Query mention edges directly to get root_id values
        // Graph traversal syntax `<-mentions.root_id` can return Null in some SurrealDB versions
        let target_thing = Thing::from(("node".to_string(), node_id.to_string()));
        let query = "SELECT root_id FROM mentions WHERE out = $target;";

        let mut response = self
            .db
            .query(query)
            .bind(("target", target_thing))
            .await
            .context("Failed to get mentioning roots")?;

        // Parse the response - each row has a root_id field
        #[derive(Debug, Deserialize)]
        struct MentionRow {
            root_id: Option<String>,
        }

        let results: Vec<MentionRow> = response
            .take(0)
            .context("Failed to extract root IDs from response")?;

        // Collect root IDs
        let mut root_ids: Vec<String> = results.into_iter().filter_map(|r| r.root_id).collect();

        // Deduplicate root IDs
        root_ids.sort();
        root_ids.dedup();

        // Fetch full node records
        let mut nodes = Vec::new();
        for root_id in root_ids {
            if let Some(node) = self.get_node(&root_id).await? {
                nodes.push(node);
            }
        }

        Ok(nodes)
    }

    pub async fn get_schema(&self, node_type: &str) -> Result<Option<Value>> {
        // Schema nodes use simple IDs (just the node type name, e.g., "date")
        // They're differentiated by node_type = "schema"
        let schema_id = node_type.to_string();
        let node = self.get_node(&schema_id).await?;
        Ok(node.map(|n| n.properties))
    }

    pub async fn update_schema(&self, node_type: &str, schema: &Value) -> Result<()> {
        // Schema nodes use simple IDs (just the node type name, e.g., "date")
        let schema_id = node_type.to_string();

        // Check if schema node exists
        // NOTE: Schema seeding uses None for source because it's internal infrastructure
        // initialization, not a client-initiated operation. These events are filtered
        // out by consumers (e.g., SSE bridge) that check source_client_id.
        if self.get_node(&schema_id).await?.is_some() {
            // Update existing schema
            let update = NodeUpdate {
                properties: Some(schema.clone()),
                ..Default::default()
            };
            self.update_node(&schema_id, update, None).await?;
        } else {
            // Create new schema node with deterministic ID
            let node = Node::new_with_id(
                schema_id,
                "schema".to_string(),
                node_type.to_string(),
                schema.clone(),
            );
            self.create_node(node, None).await?;
        }

        Ok(())
    }

    // NOTE: Old node-based embedding methods REMOVED (Issue #729)
    // The following methods operated on node.embedding_vector and node.embedding_stale:
    // - get_nodes_without_embeddings() - queried node WHERE embedding_vector IS NONE
    // - update_embedding() - set node.embedding_vector and embedding_stale = false
    // - get_nodes_with_stale_embeddings() - queried node WHERE embedding_stale = true
    //
    // Root-aggregate model now uses the `embedding` table with:
    // - get_stale_embedding_root_ids() - query embedding table for stale roots
    // - mark_root_embedding_stale() - mark embedding record stale
    // - create_stale_embedding_marker() - create stale embedding for new roots
    // - upsert_embeddings() - store embeddings in embedding table
    // NodeService.queue_root_for_embedding() orchestrates the logic.

    /// DEPRECATED: Search for nodes by embedding similarity
    ///
    /// This method is deprecated as of Issue #729. Embeddings are now stored in a
    /// dedicated `embedding` table using the root-aggregate model.
    ///
    /// Use `search_embeddings()` instead for semantic search.
    #[deprecated(
        since = "2.0.0",
        note = "Use search_embeddings() instead. Embeddings are now stored in a dedicated table."
    )]
    #[allow(clippy::unused_async)]
    pub async fn search_by_embedding(
        &self,
        _embedding: &[f32],
        _limit: i64,
        _threshold: Option<f64>,
    ) -> Result<Vec<(Node, f64)>> {
        tracing::warn!("search_by_embedding is deprecated. Use search_embeddings() instead.");
        Ok(vec![])
    }

    /// Atomic bulk update using SurrealDB transactions
    ///
    /// Updates multiple nodes in a single atomic transaction. Either all updates
    /// succeed or all fail (rollback), ensuring data consistency.
    ///
    /// # Performance Considerations
    ///
    /// - **Optimal Batch Size:** 10-100 nodes (transaction overhead minimal)
    /// - **Large Batches:** >1000 nodes may hit transaction timeout (consider chunking)
    /// - **Validation Cost:** Pre-fetches all nodes for existence check
    ///
    /// # Arguments
    ///
    /// * `updates` - Vector of (node_id, NodeUpdate) tuples to apply
    ///
    /// # Returns
    ///
    /// * `Ok(())` - All updates succeeded
    /// * `Err(_)` - Transaction failed and rolled back, or batch size exceeded limit
    ///
    /// # Examples
    ///
    /// ```rust,no_run
    /// # use nodespace_core::db::SurrealStore;
    /// # use nodespace_core::models::NodeUpdate;
    /// # use std::path::PathBuf;
    /// # #[tokio::main]
    /// # async fn main() -> Result<(), Box<dyn std::error::Error>> {
    /// # let store = SurrealStore::new(PathBuf::from("./data/surreal.db")).await?;
    /// let updates = vec![
    ///     ("node-1".to_string(), NodeUpdate {
    ///         content: Some("New content 1".to_string()),
    ///         ..Default::default()
    ///     }),
    ///     ("node-2".to_string(), NodeUpdate {
    ///         content: Some("New content 2".to_string()),
    ///         ..Default::default()
    ///     }),
    /// ];
    ///
    /// store.bulk_update(updates).await?; // All-or-nothing
    /// # Ok(())
    /// # }
    /// ```
    pub async fn bulk_update(&self, updates: Vec<(String, NodeUpdate)>) -> Result<()> {
        if updates.is_empty() {
            return Ok(());
        }

        // Prevent excessive batch sizes that could cause transaction timeouts
        const MAX_BATCH_SIZE: usize = 1000;
        if updates.len() > MAX_BATCH_SIZE {
            return Err(anyhow::anyhow!(
                "Bulk update batch size ({}) exceeds maximum ({}). Consider chunking the updates into smaller batches.",
                updates.len(),
                MAX_BATCH_SIZE
            ));
        }

        // Build transaction query
        let mut transaction_parts = vec!["BEGIN TRANSACTION;".to_string()];

        for (idx, (id, _)) in updates.iter().enumerate() {
            // Validate node exists (will fetch again later for merging values)
            self.get_node(id)
                .await?
                .ok_or_else(|| anyhow::anyhow!("Node not found: {}", id))?;

            // Generate UPDATE statement using record ID
            let update_stmt = format!(
                "UPDATE type::thing('node', $id_{idx}) SET
                    content = $content_{idx},
                    node_type = $node_type_{idx},
                    modified_at = time::now(),
                    version = version + 1;",
                idx = idx
            );
            transaction_parts.push(update_stmt);
        }

        transaction_parts.push("COMMIT TRANSACTION;".to_string());
        let transaction_query = transaction_parts.join("\n");

        // Build query with all bindings
        let mut query_builder = self.db.query(transaction_query);

        for (idx, (id, update)) in updates.iter().enumerate() {
            // Fetch current node again for building merged values
            let current = self
                .get_node(id)
                .await?
                .ok_or_else(|| anyhow::anyhow!("Node not found: {}", id))?;

            let updated_content = update.content.clone().unwrap_or(current.content);
            let updated_node_type = update.node_type.clone().unwrap_or(current.node_type);

            query_builder = query_builder
                .bind((format!("id_{}", idx), id.clone()))
                .bind((format!("content_{}", idx), updated_content))
                .bind((format!("node_type_{}", idx), updated_node_type));
        }

        query_builder
            .await
            .context("Failed to execute bulk update transaction")?;

        Ok(())
    }

    pub async fn batch_create_nodes(&self, nodes: Vec<Node>) -> Result<Vec<Node>> {
        let mut created_nodes = Vec::new();

        for node in nodes {
            let created = self.create_node(node, None).await?;
            created_nodes.push(created);
        }

        Ok(created_nodes)
    }

    pub fn close(&self) -> Result<()> {
        // SurrealDB handles cleanup automatically on drop
        Ok(())
    }

    // ========================================================================
    // Strongly-Typed Node Retrieval (Issue #673)
    // ========================================================================
    //
    // These methods provide direct deserialization from spoke tables with hub
    // data via record link, eliminating the intermediate JSON `properties` step.

    /// Get a task node with strong typing using single-query pattern
    ///
    /// Fetches spoke fields (status, priority, due_date, assignee) and hub fields
    /// (id, content, version, timestamps) in a single query via record link.
    ///
    /// # Query Pattern
    ///
    /// Column aliases use camelCase to match TaskNode's `#[serde(rename_all = "camelCase")]`:
    ///
    /// ```sql
    /// SELECT
    ///     record::id(id) AS id,
    ///     status,
    ///     priority,
    ///     due_date AS dueDate,
    ///     assignee,
    ///     node.content AS content,
    ///     node.version AS version,
    ///     node.created_at AS createdAt,
    ///     node.modified_at AS modifiedAt
    /// FROM task:`some-id`;
    /// ```
    ///
    /// # Arguments
    ///
    /// * `id` - The task node ID (without table prefix)
    ///
    /// # Returns
    ///
    /// * `Ok(Some(TaskNode))` - Task found with strongly-typed fields
    /// * `Ok(None)` - Task not found
    /// * `Err(_)` - Database or deserialization error
    ///
    /// # Examples
    ///
    /// ```rust,no_run
    /// # use nodespace_core::db::SurrealStore;
    /// # use std::path::PathBuf;
    /// # #[tokio::main]
    /// # async fn main() -> Result<(), Box<dyn std::error::Error>> {
    /// # let store = SurrealStore::new(PathBuf::from("./data/surreal.db")).await?;
    /// if let Some(task) = store.get_task_node("my-task-id").await? {
    ///     // Direct field access - no JSON parsing
    ///     println!("Status: {:?}", task.status);
    ///     println!("Priority: {:?}", task.priority);
    /// }
    /// # Ok(())
    /// # }
    /// ```
    pub async fn get_task_node(&self, id: &str) -> Result<Option<crate::models::TaskNode>> {
        // Single query: spoke fields + hub fields via record link
        // Note: Column aliases use camelCase to match TaskNode's #[serde(rename_all = "camelCase")]
        // Database stores snake_case, but serde expects camelCase for deserialization
        let query = format!(
            r#"
            SELECT
                record::id(id) AS id,
                node.node_type AS nodeType,
                status,
                priority,
                due_date AS dueDate,
                assignee,
                node.content AS content,
                node.version AS version,
                node.created_at AS createdAt,
                node.modified_at AS modifiedAt
            FROM task:`{}`;
            "#,
            id
        );

        let mut response = self
            .db
            .query(&query)
            .await
            .context(format!("Failed to query task node '{}'", id))?;

        let tasks: Vec<crate::models::TaskNode> =
            response.take(0).context("Failed to deserialize TaskNode")?;

        Ok(tasks.into_iter().next())
    }

    /// Update a task node with type-safe spoke field updates
    ///
    /// Updates the task spoke table fields (status, priority, due_date, assignee) and
    /// optionally the hub content field. Uses optimistic concurrency control (OCC)
    /// to prevent lost updates.
    ///
    /// # Transaction Pattern
    ///
    /// Updates are atomic - spoke fields and hub fields (if provided) are updated
    /// in a single transaction with OCC check:
    ///
    /// ```sql
    /// BEGIN TRANSACTION;
    /// -- OCC check
    /// LET $current = SELECT version FROM node:`id`;
    /// IF $current.version != $expected { THROW "Version mismatch" };
    /// -- Update spoke table
    /// UPDATE task:`id` SET status = $status, ...;
    /// -- Update hub (if content changed)
    /// UPDATE node:`id` SET content = $content, version = version + 1, modified_at = time::now();
    /// COMMIT;
    /// ```
    ///
    /// # Arguments
    ///
    /// * `id` - The task node ID
    /// * `expected_version` - Version for OCC check (prevents lost updates)
    /// * `update` - TaskNodeUpdate with fields to update
    ///
    /// # Returns
    ///
    /// * `Ok(TaskNode)` - Updated task with new version and modified_at
    /// * `Err(_)` - Version mismatch, node not found, or database error
    ///
    /// # Examples
    ///
    /// ```rust,no_run
    /// # use nodespace_core::db::SurrealStore;
    /// # use nodespace_core::models::{TaskNodeUpdate, TaskStatus};
    /// # use std::path::PathBuf;
    /// # #[tokio::main]
    /// # async fn main() -> Result<(), Box<dyn std::error::Error>> {
    /// # let store = SurrealStore::new(PathBuf::from("./data/surreal.db")).await?;
    /// let update = TaskNodeUpdate::new().with_status(TaskStatus::InProgress);
    /// let updated = store.update_task_node("task-123", 1, update).await?;
    /// println!("New status: {:?}", updated.status);
    /// # Ok(())
    /// # }
    /// ```
    pub async fn update_task_node(
        &self,
        id: &str,
        expected_version: i64,
        update: crate::models::TaskNodeUpdate,
    ) -> Result<crate::models::TaskNode> {
        // Build SET clauses for spoke table update
        let mut spoke_set_clauses: Vec<String> = Vec::new();

        if let Some(ref status) = update.status {
            spoke_set_clauses.push(format!("status = '{}'", status.as_str()));
        }

        if let Some(ref priority_opt) = update.priority {
            match priority_opt {
                Some(p) => spoke_set_clauses.push(format!("priority = '{}'", p.as_str())),
                None => spoke_set_clauses.push("priority = NONE".to_string()),
            }
        }

        if let Some(ref due_date_opt) = update.due_date {
            match due_date_opt {
                Some(dt) => {
                    spoke_set_clauses.push(format!("due_date = <datetime>'{}'", dt.to_rfc3339()))
                }
                None => spoke_set_clauses.push("due_date = NONE".to_string()),
            }
        }

        if let Some(ref assignee_opt) = update.assignee {
            match assignee_opt {
                // Escape single quotes to prevent SQL injection
                Some(a) => {
                    spoke_set_clauses.push(format!("assignee = '{}'", a.replace('\'', "\\'")))
                }
                None => spoke_set_clauses.push("assignee = NONE".to_string()),
            }
        }

        if let Some(ref started_at_opt) = update.started_at {
            match started_at_opt {
                Some(dt) => {
                    spoke_set_clauses.push(format!("started_at = <datetime>'{}'", dt.to_rfc3339()))
                }
                None => spoke_set_clauses.push("started_at = NONE".to_string()),
            }
        }

        if let Some(ref completed_at_opt) = update.completed_at {
            match completed_at_opt {
                Some(dt) => spoke_set_clauses
                    .push(format!("completed_at = <datetime>'{}'", dt.to_rfc3339())),
                None => spoke_set_clauses.push("completed_at = NONE".to_string()),
            }
        }

        // Build transaction
        let mut transaction_parts = vec!["BEGIN TRANSACTION;".to_string()];

        // OCC check: verify version matches
        transaction_parts.push(format!(
            r#"LET $current = (SELECT version FROM node:`{id}`);"#,
            id = id
        ));
        transaction_parts.push(format!(
            r#"IF $current[0].version != {expected_version} {{ THROW "VersionMismatch: expected {expected_version}, got " + <string>$current[0].version; }};"#,
            expected_version = expected_version
        ));

        // Update spoke table if there are spoke field changes
        // CRITICAL: Always include node link to hub for proper hub-spoke architecture
        // This ensures the spoke record has the bidirectional link even if it was
        // just created (e.g., when converting text → task via generic node type update)
        if !spoke_set_clauses.is_empty() {
            // Add the node link to ensure spoke → hub connection exists
            spoke_set_clauses.push(format!("node = node:`{}`", id));
            transaction_parts.push(format!(
                r#"UPDATE task:`{id}` SET {sets};"#,
                id = id,
                sets = spoke_set_clauses.join(", ")
            ));
        }

        // Update hub table: always bump version and modified_at, optionally update content
        // Also ensure hub → spoke link exists (data field points to spoke record)
        let hub_sets = if let Some(ref content) = update.content {
            format!(
                "content = '{}', version = version + 1, modified_at = time::now(), data = task:`{}`",
                content.replace('\'', "\\'"),
                id
            )
        } else {
            format!(
                "version = version + 1, modified_at = time::now(), data = task:`{}`",
                id
            )
        };

        transaction_parts.push(format!(
            r#"UPDATE node:`{id}` SET {sets};"#,
            id = id,
            sets = hub_sets
        ));

        transaction_parts.push("COMMIT TRANSACTION;".to_string());

        let transaction_query = transaction_parts.join("\n");

        // Execute transaction and check for errors (including IF/THROW version mismatch)
        let response = self
            .db
            .query(&transaction_query)
            .await
            .context(format!("Failed to update task node '{}'", id))?;

        // Check the response for errors - SurrealDB transactions with THROW will produce errors
        // that need to be explicitly checked via .check()
        response
            .check()
            .context(format!("Failed to update task node '{}'", id))?;

        // Fetch and return updated task node
        self.get_task_node(id)
            .await?
            .ok_or_else(|| anyhow::anyhow!("Task node '{}' not found after update", id))
    }

    /// Get a schema node with strong typing using single-query pattern
    ///
    /// Fetches spoke fields (is_core, schema_version, description, fields) and hub
    /// fields (id, content, version, timestamps) in a single query via record link.
    ///
    /// # Query Pattern
    ///
    /// Column aliases use camelCase to match SchemaNode's `#[serde(rename_all = "camelCase")]`:
    ///
    /// ```sql
    /// SELECT
    ///     record::id(id) AS id,
    ///     is_core AS isCore,
    ///     version AS schemaVersion,
    ///     description,
    ///     fields,
    ///     node.content AS content,
    ///     node.version AS version,
    ///     node.created_at AS createdAt,
    ///     node.modified_at AS modifiedAt
    /// FROM schema:`task`;
    /// ```
    ///
    /// # Arguments
    ///
    /// * `id` - The schema node ID (e.g., "task", "date")
    ///
    /// # Returns
    ///
    /// * `Ok(Some(SchemaNode))` - Schema found with strongly-typed fields
    /// * `Ok(None)` - Schema not found
    /// * `Err(_)` - Database or deserialization error
    ///
    /// # Examples
    ///
    /// ```rust,no_run
    /// # use nodespace_core::db::SurrealStore;
    /// # use std::path::PathBuf;
    /// # #[tokio::main]
    /// # async fn main() -> Result<(), Box<dyn std::error::Error>> {
    /// # let store = SurrealStore::new(PathBuf::from("./data/surreal.db")).await?;
    /// if let Some(schema) = store.get_schema_node("task").await? {
    ///     // Direct field access - no JSON parsing
    ///     println!("Is core: {}", schema.is_core);
    ///     println!("Fields: {:?}", schema.fields.len());
    /// }
    /// # Ok(())
    /// # }
    /// ```
    pub async fn get_schema_node(&self, id: &str) -> Result<Option<crate::models::SchemaNode>> {
        // Single query: spoke fields + hub fields via record link
        // Note: Column aliases use camelCase to match SchemaNode's #[serde(rename_all = "camelCase")]
        // Database stores snake_case, but serde expects camelCase for deserialization
        let query = format!(
            r#"
            SELECT
                record::id(id) AS id,
                is_core AS isCore,
                version AS schemaVersion,
                description,
                fields,
                relationships,
                node.content AS content,
                node.version AS version,
                node.created_at AS createdAt,
                node.modified_at AS modifiedAt
            FROM schema:`{}`;
            "#,
            id
        );

        let mut response = self
            .db
            .query(&query)
            .await
            .context(format!("Failed to query schema node '{}'", id))?;

        let schemas: Vec<crate::models::SchemaNode> = response
            .take(0)
            .context("Failed to deserialize SchemaNode")?;

        Ok(schemas.into_iter().next())
    }

    /// Get all schema nodes from the database
    ///
    /// Returns all schema definitions including their fields and relationships.
    /// Used by NLP discovery API to build relationship caches.
    ///
    /// # Returns
    ///
    /// Vector of all schema nodes, ordered by ID.
    ///
    /// # Example
    ///
    /// ```rust,no_run
    /// # use nodespace_core::db::SurrealStore;
    /// # use std::path::PathBuf;
    /// # #[tokio::main]
    /// # async fn main() -> Result<(), Box<dyn std::error::Error>> {
    /// # let store = SurrealStore::new(PathBuf::from("./data/surreal.db")).await?;
    /// let schemas = store.get_all_schemas().await?;
    /// for schema in schemas {
    ///     println!("{}: {} fields, {} relationships",
    ///         schema.id, schema.fields.len(), schema.relationships.len());
    /// }
    /// # Ok(())
    /// # }
    /// ```
    pub async fn get_all_schemas(&self) -> Result<Vec<crate::models::SchemaNode>> {
        let query = r#"
            SELECT
                record::id(id) AS id,
                is_core,
                version AS schema_version,
                description,
                fields,
                relationships,
                node.content AS content,
                node.version AS version,
                node.created_at AS created_at,
                node.modified_at AS modified_at
            FROM schema
            ORDER BY id;
        "#;

        let mut response = self
            .db
            .query(query)
            .await
            .context("Failed to query all schema nodes")?;

        let schemas: Vec<crate::models::SchemaNode> = response
            .take(0)
            .context("Failed to deserialize SchemaNodes")?;

        Ok(schemas)
    }

    // =========================================================================
    // Root-Aggregate Embedding Methods (Issue #729)
    // =========================================================================
    //
    // These methods work with the `embedding` table for root-aggregate semantic search.
    // Unlike the old node.embedding_vector approach, these methods:
    // - Store embeddings in a dedicated table with chunking support
    // - Track staleness for re-embedding queue
    // - Support multiple chunks per node for large content
    // =========================================================================

    /// Create or update embeddings for a root node
    ///
    /// Replaces all existing embeddings for the node with new ones.
    /// Used after content aggregation and chunking.
    ///
    /// # Arguments
    /// * `node_id` - The root node ID (without table prefix)
    /// * `embeddings` - List of embeddings to store (one per chunk)
    pub async fn upsert_embeddings(
        &self,
        node_id: &str,
        embeddings: Vec<crate::models::NewEmbedding>,
    ) -> Result<()> {
        if embeddings.is_empty() {
            return Ok(());
        }

        // Delete existing embeddings for this node
        self.db
            .query("DELETE embedding WHERE node = type::thing('node', $node_id);")
            .bind(("node_id", node_id.to_string()))
            .await
            .context("Failed to delete existing embeddings")?;

        // Insert new embeddings
        for emb in embeddings {
            let query = r#"
                CREATE embedding CONTENT {
                    node: type::thing('node', $node_id),
                    vector: $vector,
                    dimension: $dimension,
                    model_name: $model_name,
                    chunk_index: $chunk_index,
                    chunk_start: $chunk_start,
                    chunk_end: $chunk_end,
                    total_chunks: $total_chunks,
                    content_hash: $content_hash,
                    token_count: $token_count,
                    stale: false,
                    error_count: 0,
                    last_error: NONE,
                    created_at: time::now(),
                    modified_at: time::now()
                };
            "#;

            let dimension = emb.vector.len() as i32;
            self.db
                .query(query)
                .bind(("node_id", emb.node_id.clone()))
                .bind(("vector", emb.vector))
                .bind(("dimension", dimension))
                .bind((
                    "model_name",
                    emb.model_name
                        .unwrap_or_else(|| "bge-small-en-v1.5".to_string()),
                ))
                .bind(("chunk_index", emb.chunk_index))
                .bind(("chunk_start", emb.chunk_start))
                .bind(("chunk_end", emb.chunk_end))
                .bind(("total_chunks", emb.total_chunks))
                .bind(("content_hash", emb.content_hash))
                .bind(("token_count", emb.token_count))
                .await
                .context("Failed to create embedding")?;
        }

        Ok(())
    }

    /// Mark all embeddings for a node as stale
    ///
    /// Called when node content changes to trigger re-embedding.
    pub async fn mark_root_embedding_stale(&self, node_id: &str) -> Result<()> {
        self.db
            .query(
                "UPDATE embedding SET stale = true, modified_at = time::now() WHERE node = type::thing('node', $node_id);",
            )
            .bind(("node_id", node_id.to_string()))
            .await
            .context("Failed to mark root embedding as stale")?;

        Ok(())
    }

    /// Get all root node IDs with stale embeddings
    ///
    /// Returns node IDs that need re-embedding.
    pub async fn get_stale_embedding_root_ids(&self, limit: Option<i64>) -> Result<Vec<String>> {
        let sql = if limit.is_some() {
            "SELECT DISTINCT record::id(node) AS node_id FROM embedding WHERE stale = true LIMIT $limit;"
        } else {
            "SELECT DISTINCT record::id(node) AS node_id FROM embedding WHERE stale = true;"
        };

        let mut query_builder = self.db.query(sql);

        if let Some(lim) = limit {
            query_builder = query_builder.bind(("limit", lim));
        }

        #[derive(Debug, Deserialize)]
        struct NodeIdResult {
            node_id: String,
        }

        let mut response = query_builder
            .await
            .context("Failed to get stale embedding root IDs")?;

        let results: Vec<NodeIdResult> = response
            .take(0)
            .context("Failed to extract stale root IDs")?;

        Ok(results.into_iter().map(|r| r.node_id).collect())
    }

    /// Check if a node has any embeddings
    pub async fn has_embeddings(&self, node_id: &str) -> Result<bool> {
        #[derive(Debug, Deserialize)]
        struct CountResult {
            count: i64,
        }

        let mut response = self
            .db
            .query("SELECT count() AS count FROM embedding WHERE node = type::thing('node', $node_id) GROUP ALL;")
            .bind(("node_id", node_id.to_string()))
            .await
            .context("Failed to check for embeddings")?;

        let results: Vec<CountResult> = response.take(0).unwrap_or_default();

        Ok(results.first().map(|r| r.count > 0).unwrap_or(false))
    }

    /// Delete all embeddings for a node
    ///
    /// Called when a node is deleted.
    pub async fn delete_embeddings(&self, node_id: &str) -> Result<()> {
        self.db
            .query("DELETE embedding WHERE node = type::thing('node', $node_id);")
            .bind(("node_id", node_id.to_string()))
            .await
            .context("Failed to delete embeddings")?;

        Ok(())
    }

    /// Record an embedding error
    ///
    /// Increments error count and stores the error message.
    pub async fn record_embedding_error(&self, node_id: &str, error: &str) -> Result<()> {
        self.db
            .query(
                r#"
                UPDATE embedding SET
                    error_count = error_count + 1,
                    last_error = $error,
                    modified_at = time::now()
                WHERE node = type::thing('node', $node_id);
                "#,
            )
            .bind(("node_id", node_id.to_string()))
            .bind(("error", error.to_string()))
            .await
            .context("Failed to record embedding error")?;

        Ok(())
    }

    /// Search embeddings by vector similarity
    ///
    /// Returns the best-matching chunk per node, grouped by node.
    /// Uses SurrealDB's native vector similarity functions.
    ///
    /// # Arguments
    /// * `query_vector` - The query embedding vector
    /// * `limit` - Maximum number of results
    /// * `threshold` - Minimum similarity threshold (0.0-1.0)
    pub async fn search_embeddings(
        &self,
        query_vector: &[f32],
        limit: i64,
        threshold: Option<f64>,
    ) -> Result<Vec<crate::models::EmbeddingSearchResult>> {
        let min_similarity = threshold.unwrap_or(0.5);

        // Query to get best similarity per node across all chunks
        let query = r#"
            SELECT
                record::id(node) AS node_id,
                math::max(vector::similarity::cosine(vector, $query_vector)) AS similarity
            FROM embedding
            WHERE stale = false
            GROUP BY node
            HAVING similarity > $threshold
            ORDER BY similarity DESC
            LIMIT $limit;
        "#;

        let mut response = self
            .db
            .query(query)
            .bind(("query_vector", query_vector.to_vec()))
            .bind(("threshold", min_similarity))
            .bind(("limit", limit))
            .await
            .context("Failed to execute embedding search")?;

        let results: Vec<crate::models::EmbeddingSearchResult> = response
            .take(0)
            .context("Failed to extract embedding search results")?;

        Ok(results)
    }

    /// Create a stale embedding marker for a new root node
    ///
    /// Creates an empty embedding record marked as stale to queue it for processing.
    /// Used when a new root node is created that should be embedded.
    pub async fn create_stale_embedding_marker(&self, node_id: &str) -> Result<()> {
        let query = r#"
            CREATE embedding CONTENT {
                node: type::thing('node', $node_id),
                vector: [],
                dimension: 384,
                model_name: 'bge-small-en-v1.5',
                chunk_index: 0,
                chunk_start: 0,
                chunk_end: NONE,
                total_chunks: 1,
                content_hash: NONE,
                token_count: NONE,
                stale: true,
                error_count: 0,
                last_error: NONE,
                created_at: time::now(),
                modified_at: time::now()
            };
        "#;

        self.db
            .query(query)
            .bind(("node_id", node_id.to_string()))
            .await
            .context("Failed to create stale embedding marker")?;

        Ok(())
    }
}

#[cfg(test)]
#[allow(deprecated)] // Tests use deprecated search_by_embedding for backward compatibility testing
mod tests {
    use super::*;
    use serde_json::json;
    use tempfile::TempDir;

    /// Test helper to create a SurrealStore with schemas seeded
    ///
    /// Since schema seeding moved to NodeService (Issue #704), we use NodeService
    /// to seed schemas. The new() method now takes &mut Arc to update caches
    /// incrementally during seeding - no rebuild needed.
    async fn create_test_store() -> Result<(Arc<SurrealStore>, TempDir)> {
        use crate::services::NodeService;

        let temp_dir = TempDir::new()?;
        let db_path = temp_dir.path().join("test_surreal.db");
        let mut store_arc = Arc::new(SurrealStore::new(db_path).await?);

        // Seed schemas via NodeService (Issue #704)
        // NodeService::new() takes &mut Arc to update caches incrementally during seeding
        let _ = NodeService::new(&mut store_arc)
            .await
            .map_err(|e| anyhow::anyhow!("Failed to initialize NodeService: {}", e))?;

        // Caches are now populated by NodeService::new() - no rebuild needed!
        Ok((store_arc, temp_dir))
    }

    #[tokio::test]
    async fn test_create_and_get_node() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        let node = Node::new("text".to_string(), "Test content".to_string(), json!({}));

        let created = store.create_node(node.clone(), None).await?;
        assert_eq!(created.id, node.id);
        assert_eq!(created.content, "Test content");

        let fetched = store.get_node(&node.id).await?;
        assert!(fetched.is_some());
        assert_eq!(fetched.unwrap().id, node.id);

        Ok(())
    }

    #[tokio::test]
    async fn test_update_node() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        let node = Node::new(
            "text".to_string(),
            "Original content".to_string(),
            json!({}),
        );

        let created = store.create_node(node.clone(), None).await?;

        let update = NodeUpdate {
            content: Some("Updated content".to_string()),
            ..Default::default()
        };

        let updated = store.update_node(&created.id, update, None).await?;
        assert_eq!(updated.content, "Updated content");

        Ok(())
    }

    #[tokio::test]
    async fn test_delete_node() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        let node = Node::new("text".to_string(), "Test content".to_string(), json!({}));

        let created = store.create_node(node.clone(), None).await?;

        let result = store.delete_node(&created.id, None).await?;
        assert!(result.existed);

        let fetched = store.get_node(&created.id).await?;
        assert!(fetched.is_none());

        Ok(())
    }

    #[tokio::test]
    async fn test_schema_operations() -> Result<()> {
        use crate::models::schema::{SchemaField, SchemaProtectionLevel};

        let (store, _temp_dir) = create_test_store().await?;

        // Create schema properties with fields containing SchemaProtectionLevel enum
        // This tests that enums are stored and retrieved correctly without stringification
        let schema_props = serde_json::json!({
            "isCore": false,
            "version": 1,
            "description": "Test task schema",
            "fields": [
                {
                    "name": "status",
                    "type": "enum",
                    "protection": "core",
                    "coreValues": [
                        { "value": "open", "label": "Open" },
                        { "value": "in_progress", "label": "In Progress" },
                        { "value": "done", "label": "Done" }
                    ],
                    "indexed": true,
                    "required": true,
                    "extensible": true,
                    "default": "open",
                    "description": "Task status"
                }
            ]
        });

        store.update_schema("task", &schema_props).await?;

        // Fetch and verify the schema was stored correctly
        let fetched = store.get_schema("task").await?;
        assert!(fetched.is_some(), "Schema should be fetched");

        let fetched_value = fetched.unwrap();

        // Verify the schema was stored and retrieved correctly
        assert_eq!(fetched_value["version"], 1);
        assert_eq!(fetched_value["description"], "Test task schema");

        // Parse and verify fields with SchemaProtectionLevel
        let fields: Vec<SchemaField> = serde_json::from_value(fetched_value["fields"].clone())?;
        assert_eq!(fields.len(), 1);
        assert_eq!(fields[0].name, "status");
        // Key assertion: SchemaProtectionLevel enum correctly deserialized
        assert_eq!(fields[0].protection, SchemaProtectionLevel::Core);

        Ok(())
    }

    // ============================================================================
    // NOTE: Old per-node embedding tests REMOVED (Issue #729)
    //
    // The following tests were removed as they tested the deprecated per-node
    // embedding model (node.embedding_vector, update_embedding(), search_by_embedding):
    // - test_search_empty_database
    // - test_search_with_similar_nodes
    // - test_search_with_threshold_filter
    // - test_search_respects_limit
    // - test_search_performance_1k_nodes
    // - test_search_with_real_nlp_embeddings
    // - test_search_performance_10k_nodes
    //
    // The new root-aggregate embedding model uses the `embedding` table.
    // See NodeEmbeddingService tests for the new search functionality.
    // ============================================================================

    // ============================================================================
    // Atomic Transactional Operations Tests (Issue #532)
    // ============================================================================

    #[tokio::test]
    async fn test_create_child_node_atomic_success() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        // Create parent node
        let parent = Node::new("text".to_string(), "Parent".to_string(), json!({}));
        let parent = store.create_node(parent, None).await?;

        // Create child atomically
        let child = store
            .create_child_node_atomic(&parent.id, "text", "Child content", json!({}), None)
            .await?;

        // Verify child was created
        assert_eq!(child.content, "Child content");
        assert_eq!(child.node_type, "text");

        // Verify parent-child edge exists
        let children = store.get_children(Some(&parent.id)).await?;
        assert_eq!(children.len(), 1);
        assert_eq!(children[0].id, child.id);

        Ok(())
    }

    #[tokio::test]
    async fn test_create_child_node_atomic_with_properties() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        // Create parent
        let parent = Node::new("text".to_string(), "Parent".to_string(), json!({}));
        let parent = store.create_node(parent, None).await?;

        // Create task child atomically with properties
        let properties = json!({
            "status": "TODO",
            "priority": "HIGH"
        });

        let child = store
            .create_child_node_atomic(&parent.id, "task", "Task content", properties, None)
            .await?;

        // Verify properties were set
        let fetched = store.get_node(&child.id).await?.unwrap();
        assert_eq!(fetched.properties["status"], "TODO");
        assert_eq!(fetched.properties["priority"], "HIGH");

        Ok(())
    }

    #[tokio::test]
    async fn test_create_child_node_atomic_rollback_on_failure() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        // Count initial nodes (seeded core schemas: task, date, text, header, code-block, quote-block, ordered-list = 7)
        let initial_nodes = store.query_nodes(NodeQuery::new()).await?;
        let initial_count = initial_nodes.len();

        // Try to create child with non-existent parent (should fail)
        let result = store
            .create_child_node_atomic("non-existent-parent", "text", "Child", json!({}), None)
            .await;

        assert!(result.is_err());

        // Verify no new nodes were created (orphan nodes would increase the count)
        let final_nodes = store.query_nodes(NodeQuery::new()).await?;
        assert_eq!(
            final_nodes.len(),
            initial_count,
            "No nodes should be created after failed transaction - expected {} nodes, got {}",
            initial_count,
            final_nodes.len()
        );

        Ok(())
    }

    #[tokio::test]
    async fn test_move_node_atomic_success() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        // Create parent1, parent2, and child
        let parent1 = store
            .create_node(
                Node::new("text".to_string(), "Parent 1".to_string(), json!({})),
                None,
            )
            .await?;
        let parent2 = store
            .create_node(
                Node::new("text".to_string(), "Parent 2".to_string(), json!({})),
                None,
            )
            .await?;
        let child = store
            .create_child_node_atomic(&parent1.id, "text", "Child", json!({}), None)
            .await?;

        // Verify child is under parent1
        let children1 = store.get_children(Some(&parent1.id)).await?;
        assert_eq!(children1.len(), 1);

        // Move child to parent2 atomically
        store.move_node(&child.id, Some(&parent2.id), None).await?;

        // Verify child is now under parent2
        let children1_after = store.get_children(Some(&parent1.id)).await?;
        let children2_after = store.get_children(Some(&parent2.id)).await?;

        assert_eq!(children1_after.len(), 0, "Parent1 should have no children");
        assert_eq!(children2_after.len(), 1, "Parent2 should have 1 child");
        assert_eq!(children2_after[0].id, child.id);

        Ok(())
    }

    #[tokio::test]
    async fn test_move_node_atomic_to_root() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        // Create parent and child
        let parent = store
            .create_node(
                Node::new("text".to_string(), "Parent".to_string(), json!({})),
                None,
            )
            .await?;
        let child = store
            .create_child_node_atomic(&parent.id, "text", "Child", json!({}), None)
            .await?;

        // Move child to root
        store.move_node(&child.id, None, None).await?;

        // Verify child is a root node
        let parent_children = store.get_children(Some(&parent.id)).await?;
        let root_nodes = store.get_children(None).await?;

        assert_eq!(parent_children.len(), 0);
        assert!(root_nodes.iter().any(|n| n.id == child.id));

        Ok(())
    }

    #[tokio::test]
    async fn test_move_node_atomic_prevents_cycles() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        // Create parent and child
        let parent = store
            .create_node(
                Node::new("text".to_string(), "Parent".to_string(), json!({})),
                None,
            )
            .await?;
        let child = store
            .create_child_node_atomic(&parent.id, "text", "Child", json!({}), None)
            .await?;

        // Try to move parent under child (would create cycle)
        let result = store.move_node(&parent.id, Some(&child.id), None).await;

        assert!(
            result.is_err(),
            "Moving parent under child should fail (cycle detection)"
        );

        Ok(())
    }

    #[tokio::test]
    async fn test_delete_node_cascade_atomic_success() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        // Create parent and child
        let parent = store
            .create_node(
                Node::new("text".to_string(), "Parent".to_string(), json!({})),
                None,
            )
            .await?;
        let child = store
            .create_child_node_atomic(&parent.id, "text", "Child", json!({}), None)
            .await?;

        // Delete parent (should cascade delete edges)
        let result = store.delete_node_cascade_atomic(&parent.id, None).await?;
        assert!(result.existed);

        // Verify parent was deleted
        let parent_fetched = store.get_node(&parent.id).await?;
        assert!(parent_fetched.is_none());

        // Verify child still exists (cascade doesn't delete children, only edges)
        let child_fetched = store.get_node(&child.id).await?;
        assert!(child_fetched.is_some());

        // Verify child is now a root node (no parent edge)
        let root_nodes = store.get_children(None).await?;
        assert!(root_nodes.iter().any(|n| n.id == child.id));

        Ok(())
    }

    #[tokio::test]
    async fn test_delete_node_cascade_atomic_idempotent() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        // Delete non-existent node (should succeed idempotently)
        let result = store
            .delete_node_cascade_atomic("non-existent-id", None)
            .await?;
        assert!(!result.existed);

        Ok(())
    }

    #[tokio::test]
    async fn test_delete_node_cascade_atomic_with_task() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        // Create task node with properties
        let task = Node::new(
            "task".to_string(),
            "Task content".to_string(),
            json!({"status": "TODO"}),
        );
        let task = store.create_node(task, None).await?;

        // Delete task (should delete both node and task-specific record)
        let result = store.delete_node_cascade_atomic(&task.id, None).await?;
        assert!(result.existed);

        // Verify complete deletion
        let fetched = store.get_node(&task.id).await?;
        assert!(fetched.is_none());

        Ok(())
    }

    #[tokio::test]
    async fn test_switch_node_type_atomic_text_to_task() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        // Create text node
        let node = store
            .create_node(
                Node::new("text".to_string(), "Original text".to_string(), json!({})),
                None,
            )
            .await?;

        // Switch to task type atomically
        let updated = store
            .switch_node_type_atomic(
                &node.id,
                "task",
                json!({"status": "TODO", "priority": "HIGH"}),
                None,
            )
            .await?;

        // Verify type switch
        assert_eq!(updated.node_type, "task");
        assert_eq!(updated.properties["status"], "TODO");
        assert_eq!(updated.properties["priority"], "HIGH");

        // Verify content preserved
        assert_eq!(updated.content, "Original text");

        Ok(())
    }

    #[tokio::test]
    async fn test_switch_node_type_atomic_task_to_text() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        // Create task node
        let task = store
            .create_node(
                Node::new(
                    "task".to_string(),
                    "Task content".to_string(),
                    json!({"status": "done"}),
                ),
                None,
            )
            .await?;

        // Switch to text type atomically
        let updated = store
            .switch_node_type_atomic(&task.id, "text", json!({}), None)
            .await?;

        // Verify type switch
        assert_eq!(updated.node_type, "text");

        // Verify content preserved
        assert_eq!(updated.content, "Task content");

        Ok(())
    }

    #[tokio::test]
    async fn test_switch_node_type_atomic_preserves_variants() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        // Create text node
        let node = store
            .create_node(
                Node::new("text".to_string(), "Content".to_string(), json!({})),
                None,
            )
            .await?;

        // Switch to task
        store
            .switch_node_type_atomic(&node.id, "task", json!({"status": "TODO"}), None)
            .await?;

        // Switch back to text
        let _final_node = store
            .switch_node_type_atomic(&node.id, "text", json!({}), None)
            .await?;

        // Fetch with properties to check variants map
        let fetched = store.get_node(&node.id).await?.unwrap();

        // Variants should be preserved (this is implementation detail, test structure exists)
        assert_eq!(fetched.node_type, "text");

        Ok(())
    }

    #[tokio::test]
    async fn test_atomic_operations_performance() -> Result<()> {
        let (store, _temp_dir) = create_test_store().await?;

        // Create parent
        let parent = store
            .create_node(
                Node::new("text".to_string(), "Parent".to_string(), json!({})),
                None,
            )
            .await?;

        // Measure create_child_node_atomic performance with multiple iterations
        // to account for variance and get statistically reliable results
        const ITERATIONS: usize = 20;
        let mut measurements = Vec::with_capacity(ITERATIONS);

        for i in 0..ITERATIONS {
            let start = std::time::Instant::now();
            let _child = store
                .create_child_node_atomic(
                    &parent.id,
                    "text",
                    &format!("Child{}", i),
                    json!({}),
                    None,
                )
                .await?;
            measurements.push(start.elapsed());
        }

        // Calculate P95 percentile (95th percentile of measurements)
        measurements.sort();
        let p95_index = (ITERATIONS * 95) / 100;
        let p95_latency = measurements[p95_index];

        // Performance target: P95 <50ms for atomic operations when running in test suite
        // Original target was 15ms (Issue #532) but that's too tight when running alongside
        // 540+ other tests due to CPU contention. 50ms allows for system load variance while
        // still catching actual performance regressions.
        assert!(
            p95_latency.as_millis() < 50,
            "create_child_node_atomic P95 latency should be <50ms, got {:?}. Measurements (ms): {:?}",
            p95_latency,
            measurements
                .iter()
                .map(|d| d.as_millis())
                .collect::<Vec<_>>()
        );

        Ok(())
    }

    // Tests for the adjacency list strategy (recursive graph traversal)
    // Uses SurrealDB's .{..}(->edge->target) syntax for recursive queries

    #[tokio::test]
    async fn test_get_nodes_in_subtree_returns_descendants() -> Result<()> {
        let (store, _temp) = create_test_store().await?;

        // Create a tree structure: root -> child -> grandchild
        let root = Node::new("text".to_string(), "Root".to_string(), json!({}));
        let child = Node::new("text".to_string(), "Child".to_string(), json!({}));
        let grandchild = Node::new("text".to_string(), "Grandchild".to_string(), json!({}));

        store.create_node(root.clone(), None).await?;
        store.create_node(child.clone(), None).await?;
        store.create_node(grandchild.clone(), None).await?;

        // Create edges: root -> child -> grandchild
        store.move_node(&child.id, Some(&root.id), None).await?;
        store
            .move_node(&grandchild.id, Some(&child.id), None)
            .await?;

        // Get nodes in subtree of root - should include child and grandchild
        let subtree_nodes = store.get_nodes_in_subtree(&root.id).await?;

        assert_eq!(
            subtree_nodes.len(),
            2,
            "Should have 2 descendants (child and grandchild)"
        );
        let ids: Vec<_> = subtree_nodes.iter().map(|n| n.id.clone()).collect();
        assert!(ids.contains(&child.id), "Should contain child");
        assert!(ids.contains(&grandchild.id), "Should contain grandchild");

        Ok(())
    }

    #[tokio::test]
    async fn test_get_nodes_in_subtree_leaf_node_returns_empty() -> Result<()> {
        let (store, _temp) = create_test_store().await?;

        // Create a leaf node with no children
        let leaf = Node::new("text".to_string(), "Leaf".to_string(), json!({}));
        store.create_node(leaf.clone(), None).await?;

        // Get nodes in subtree of leaf - should return empty vec
        let subtree_nodes = store.get_nodes_in_subtree(&leaf.id).await?;

        assert!(
            subtree_nodes.is_empty(),
            "Leaf node should have no descendants"
        );

        Ok(())
    }

    #[tokio::test]
    async fn test_get_edges_in_subtree_returns_subtree_edges() -> Result<()> {
        let (store, _temp) = create_test_store().await?;

        // Create a tree structure: root -> child -> grandchild
        let root = Node::new("text".to_string(), "Root".to_string(), json!({}));
        let child = Node::new("text".to_string(), "Child".to_string(), json!({}));
        let grandchild = Node::new("text".to_string(), "Grandchild".to_string(), json!({}));

        store.create_node(root.clone(), None).await?;
        store.create_node(child.clone(), None).await?;
        store.create_node(grandchild.clone(), None).await?;

        // Create edges: root -> child -> grandchild
        store.move_node(&child.id, Some(&root.id), None).await?;
        store
            .move_node(&grandchild.id, Some(&child.id), None)
            .await?;

        // Get edges in subtree of root - should include both edges
        let subtree_edges = store.get_edges_in_subtree(&root.id).await?;

        assert_eq!(subtree_edges.len(), 2, "Should have 2 edges in subtree");

        // Verify the edges are correct
        let edge_pairs: Vec<_> = subtree_edges
            .iter()
            .map(|e| (e.in_node.clone(), e.out_node.clone()))
            .collect();
        assert!(
            edge_pairs.contains(&(root.id.clone(), child.id.clone())),
            "Should contain root->child edge"
        );
        assert!(
            edge_pairs.contains(&(child.id.clone(), grandchild.id.clone())),
            "Should contain child->grandchild edge"
        );

        Ok(())
    }

    // ==================== Mention Autocomplete Tests ====================

    #[tokio::test]
    async fn test_mention_autocomplete_excludes_date_and_schema_types() -> Result<()> {
        let (store, _temp) = create_test_store().await?;

        // Create nodes of different types with matching content
        let text_node = Node::new(
            "text".to_string(),
            "searchable content".to_string(),
            json!({}),
        );
        let date_node = Node::new(
            "date".to_string(),
            "searchable content".to_string(),
            json!({}),
        );
        let schema_node = Node::new(
            "schema".to_string(),
            "searchable content".to_string(),
            json!({}),
        );
        let task_node = Node::new(
            "task".to_string(),
            "searchable content".to_string(),
            json!({}),
        );

        store.create_node(text_node.clone(), None).await?;
        store.create_node(date_node.clone(), None).await?;
        store.create_node(schema_node.clone(), None).await?;
        store.create_node(task_node.clone(), None).await?;

        // Search for matching content
        let results = store.mention_autocomplete("searchable", None).await?;

        // Should find text and task, but NOT date or schema
        let result_ids: Vec<_> = results.iter().map(|n| &n.id).collect();
        assert!(
            result_ids.contains(&&text_node.id),
            "Should include text node"
        );
        assert!(
            result_ids.contains(&&task_node.id),
            "Should include task node"
        );
        assert!(
            !result_ids.contains(&&date_node.id),
            "Should NOT include date node"
        );
        assert!(
            !result_ids.contains(&&schema_node.id),
            "Should NOT include schema node"
        );

        Ok(())
    }

    #[tokio::test]
    async fn test_mention_autocomplete_text_types_only_root_nodes() -> Result<()> {
        let (store, _temp) = create_test_store().await?;

        // Create root text nodes
        let root_text = Node::new("text".to_string(), "findable root".to_string(), json!({}));
        let root_header = Node::new(
            "header".to_string(),
            "findable header".to_string(),
            json!({}),
        );
        let root_code = Node::new(
            "code-block".to_string(),
            "findable code".to_string(),
            json!({}),
        );
        let root_quote = Node::new(
            "quote-block".to_string(),
            "findable quote root".to_string(),
            json!({}),
        );
        let root_ordered = Node::new(
            "ordered-list".to_string(),
            "1. findable ordered".to_string(),
            json!({}),
        );

        // Create nested text nodes (have parent)
        let parent = Node::new("text".to_string(), "parent node".to_string(), json!({}));
        let nested_text = Node::new("text".to_string(), "findable nested".to_string(), json!({}));
        let nested_quote = Node::new(
            "quote-block".to_string(),
            "findable quote".to_string(),
            json!({}),
        );

        store.create_node(root_text.clone(), None).await?;
        store.create_node(root_header.clone(), None).await?;
        store.create_node(root_code.clone(), None).await?;
        store.create_node(root_quote.clone(), None).await?;
        store.create_node(root_ordered.clone(), None).await?;
        store.create_node(parent.clone(), None).await?;
        store.create_node(nested_text.clone(), None).await?;
        store.create_node(nested_quote.clone(), None).await?;

        // Make nested nodes children of parent
        store
            .move_node(&nested_text.id, Some(&parent.id), None)
            .await?;
        store
            .move_node(&nested_quote.id, Some(&parent.id), None)
            .await?;

        // Search for "findable"
        let results = store.mention_autocomplete("findable", None).await?;

        let result_ids: Vec<_> = results.iter().map(|n| &n.id).collect();

        // Root text-type nodes should be included
        assert!(
            result_ids.contains(&&root_text.id),
            "Should include root text node"
        );
        assert!(
            result_ids.contains(&&root_header.id),
            "Should include root header node"
        );
        assert!(
            result_ids.contains(&&root_code.id),
            "Should include root code-block node"
        );
        assert!(
            result_ids.contains(&&root_quote.id),
            "Should include root quote-block node"
        );
        assert!(
            result_ids.contains(&&root_ordered.id),
            "Should include root ordered-list node"
        );

        // Nested text-type nodes should NOT be included
        assert!(
            !result_ids.contains(&&nested_text.id),
            "Should NOT include nested text node"
        );
        assert!(
            !result_ids.contains(&&nested_quote.id),
            "Should NOT include nested quote-block node"
        );

        Ok(())
    }

    #[tokio::test]
    async fn test_mention_autocomplete_non_text_types_include_nested() -> Result<()> {
        let (store, _temp) = create_test_store().await?;

        // Create parent node
        let parent = Node::new("text".to_string(), "parent node".to_string(), json!({}));

        // Create task nodes - one root, one nested
        let root_task = Node::new(
            "task".to_string(),
            "findme root task".to_string(),
            json!({}),
        );
        let nested_task = Node::new(
            "task".to_string(),
            "findme nested task".to_string(),
            json!({}),
        );

        // Create query node - nested
        let nested_query = Node::new(
            "query".to_string(),
            "findme nested query".to_string(),
            json!({}),
        );

        store.create_node(parent.clone(), None).await?;
        store.create_node(root_task.clone(), None).await?;
        store.create_node(nested_task.clone(), None).await?;
        store.create_node(nested_query.clone(), None).await?;

        // Make tasks and query children of parent
        store
            .move_node(&nested_task.id, Some(&parent.id), None)
            .await?;
        store
            .move_node(&nested_query.id, Some(&parent.id), None)
            .await?;

        // Search for "findme"
        let results = store.mention_autocomplete("findme", None).await?;

        let result_ids: Vec<_> = results.iter().map(|n| &n.id).collect();

        // Both root and nested task/query nodes should be included
        assert!(
            result_ids.contains(&&root_task.id),
            "Should include root task"
        );
        assert!(
            result_ids.contains(&&nested_task.id),
            "Should include nested task"
        );
        assert!(
            result_ids.contains(&&nested_query.id),
            "Should include nested query"
        );

        Ok(())
    }

    #[tokio::test]
    async fn test_mention_autocomplete_case_insensitive() -> Result<()> {
        let (store, _temp) = create_test_store().await?;

        let node1 = Node::new(
            "text".to_string(),
            "UPPERCASE content".to_string(),
            json!({}),
        );
        let node2 = Node::new(
            "task".to_string(),
            "lowercase content".to_string(),
            json!({}),
        );
        let node3 = Node::new(
            "text".to_string(),
            "MixedCase Content".to_string(),
            json!({}),
        );

        store.create_node(node1.clone(), None).await?;
        store.create_node(node2.clone(), None).await?;
        store.create_node(node3.clone(), None).await?;

        // Search with lowercase should find all
        let results = store.mention_autocomplete("content", None).await?;
        assert_eq!(results.len(), 3, "Should find all 3 nodes with 'content'");

        // Search with uppercase should also find all
        let results = store.mention_autocomplete("CONTENT", None).await?;
        assert_eq!(
            results.len(),
            3,
            "Should find all 3 nodes with 'CONTENT' (case insensitive)"
        );

        Ok(())
    }

    #[tokio::test]
    async fn test_mention_autocomplete_respects_limit() -> Result<()> {
        let (store, _temp) = create_test_store().await?;

        // Create multiple matching nodes
        for i in 0..5 {
            let node = Node::new(
                "task".to_string(),
                format!("searchterm item {}", i),
                json!({}),
            );
            store.create_node(node, None).await?;
        }

        // Search with limit
        let results = store.mention_autocomplete("searchterm", Some(3)).await?;
        assert_eq!(results.len(), 3, "Should respect limit of 3");

        // Default limit (10) with fewer results
        let results = store.mention_autocomplete("searchterm", None).await?;
        assert_eq!(
            results.len(),
            5,
            "Should return all 5 when under default limit"
        );

        Ok(())
    }

    #[tokio::test]
    async fn test_mention_autocomplete_no_results() -> Result<()> {
        let (store, _temp) = create_test_store().await?;

        let node = Node::new("text".to_string(), "some content".to_string(), json!({}));
        store.create_node(node, None).await?;

        // Search for non-existent term
        let results = store.mention_autocomplete("nonexistent", None).await?;
        assert!(results.is_empty(), "Should return empty for no matches");

        Ok(())
    }
}
