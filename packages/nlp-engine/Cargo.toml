[package]
name = "nodespace-nlp-engine"
version = "0.1.0"
description = "NodeSpace unified vector embedding service using llama.cpp"
authors.workspace = true
edition.workspace = true
license.workspace = true
repository.workspace = true

[dependencies]
# llama.cpp for embeddings via Rust bindings
# Supports Metal (macOS), CUDA (NVIDIA), Vulkan (cross-platform)
# Pinned to 0.1.132+ for LlamaCppError naming (was LLamaCppError in older versions)
llama-cpp-2 = { version = ">=0.1.132", optional = true }

# Caching layer
lru = "0.12"

# Workspace dependencies
serde = { workspace = true }
serde_json = { workspace = true }
tokio = { workspace = true }
anyhow = { workspace = true }
thiserror = { workspace = true }

# Logging
tracing = "0.1"

# Path utilities
dirs = "5.0"

[dev-dependencies]
tokio-test = { workspace = true }
criterion = { workspace = true }
tempfile = "3.0"
futures = "0.3"

[features]
default = ["embedding-service"]
embedding-service = ["llama-cpp-2"]

# GPU acceleration features
cuda = ["llama-cpp-2/cuda"]
metal = ["llama-cpp-2/metal"]
vulkan = ["llama-cpp-2/vulkan"]

# Platform-specific: Enable Metal acceleration on macOS by default
[target.'cfg(target_os = "macos")'.dependencies]
llama-cpp-2 = { version = ">=0.1.132", features = ["metal"], optional = true }
